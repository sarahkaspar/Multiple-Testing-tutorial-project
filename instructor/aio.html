<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Multiple Testing: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="../assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="../assets/styles.css">
<script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="manifest" href="../site.webmanifest">
<link rel="mask-icon" href="../safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="black">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
<li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul>
</li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='../aio.html';">Learner View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Multiple Testing
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Multiple Testing
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<hr>
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="../instructor/aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Multiple Testing
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text">
<li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul>
</li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../aio.html">Learner View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->

            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="What-is-multiple-testing.html">1. What is multiple testing</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="Types-of-errors.html">2. Types of errors and error rates</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="Family-wise-error-rate.html">3. Family-wise error rate</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="False-discovery-rate.html">4. False discovery rate</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="Pairwise-comparisons.html">5. Pairwise comparisons</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="Summary.html">6. Summary</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="References.html">7. References</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr>
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width resources">
<a href="../instructor/aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">

            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-What-is-multiple-testing"><p>Content from <a href="What-is-multiple-testing.html">What is multiple testing</a></p>
<hr>
<p>Last updated on 2024-07-23 |

        <a href="https://example.com/FIXME-template-source/edit/main/episodes/What-is-multiple-testing.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is multiple testing?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Define multiple testing</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="what-is-multiple-testing">What is multiple testing<a class="anchor" aria-label="anchor" href="#what-is-multiple-testing"></a>
</h1>
<p>Suppose the prevalence of a disease in the general population is 4%.
In this population, lives a group of individuals who have all been
exposed to air pollution. Concerned about their health, we decide to
embark on a quest to uncover whether being exposed to air pollution
influenced the risk of contracting this disease.</p>
<figure><img src="../fig/01-Disease-prevalence.png" alt="Disease prevalence in a population- Illustrating the proportion of individuals affected by the disease as 4% of the population" class="figure mx-auto d-block"><div class="figcaption">Disease prevalence in a population- Illustrating
the proportion of individuals affected by the disease as 4% of the
population</div>
</figure><div class="section level2">
<h2 id="setting-the-null-and-alternative-hypothesis">Setting the null and alternative hypothesis<a class="anchor" aria-label="anchor" href="#setting-the-null-and-alternative-hypothesis"></a>
</h2>
<p>We would like to conduct a hypothesis test to find out whether the
prevalence in the test group differs from the known 4%.</p>
<div class="section level3">
<h3 id="null-hypothesis-h_0">Null Hypothesis (<span class="math inline">\(H_0\)</span>)<a class="anchor" aria-label="anchor" href="#null-hypothesis-h_0"></a>
</h3>
<p>The prevalence of the disease within test group exposed to air
pollution is the same as the known prevalence in the general population
(4%). This means that the proportion of individuals exposed to air
pollution in the test group who have the disease is also 4%.</p>
</div>
<div class="section level3">
<h3 id="alternative-hypothesis-h_1">Alternative Hypothesis (<span class="math inline">\(H_1\)</span>)<a class="anchor" aria-label="anchor" href="#alternative-hypothesis-h_1"></a>
</h3>
<p>The prevalence of the disease within the test group exposed to air
pollution is different from the known prevalence in the general
population. This means that the proportion of individuals exposed to air
pollution in the test group who have the disease is either higher or
lower than 4%.</p>
</div>
</div>
<div class="section level2">
<h2 id="data-collection-and-testing">Data collection and testing<a class="anchor" aria-label="anchor" href="#data-collection-and-testing"></a>
</h2>
<p>We assemble a group of 100 individuals who have been exposed to air
pollution (test group) from the population and each individual is
carefully examined, checking for any signs of the disease. Out of the
100 individuals, we discover that 9 of them were indeed suffering from
the disease, so the <em>observed</em> proportion is 9%. This is
different from 4%, but we are not satisfied with just this knowledge,
since the observed difference in proportions could be due to chance. We
want to know if this prevalence of the disease within the group exposed
to air pollution was significantly different from the population’s
average, meaning that it’s very unlikely to observe this difference just
by chance. So, we decide to perform binomial test (please refer back to
binomial tests tutorial) <a href="%22https://sarahkaspar.github.io/biostatistics-course/03-binomial.html%22" class="external-link">The
binomial distribution</a>. With this test, we could compare the observed
prevalence within our group that has been exposed to air pollution to
the known prevalence in the entire population.</p>
<p>We set our significance level (α) beforehand, typically at 0.05, to
determine whether the results are statistically significant.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#For known parameters (n=100, p=0.04), we calculates the the chances of getting the 9 individuals that indeed suffered from the disease. </span></span>
<span><span class="va">n</span> <span class="op">=</span> <span class="fl">100</span> <span class="co"># number of test persons</span></span>
<span><span class="va">p</span> <span class="op">=</span> <span class="fl">0.04</span> <span class="co"># Known prevalence of the disease in the general population</span></span>
<span><span class="fu">dbinom</span><span class="op">(</span>x<span class="op">=</span><span class="fl">9</span>, size<span class="op">=</span><span class="va">n</span>, prob<span class="op">=</span><span class="va">p</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 0.01214746</code></pre>
</div>
<p>The P-value we get, is the probability of obtaining extreme outcome,
assuming that the null hypothesis is true. Therefore, the binomial test
calculates the probability of observing the obtained (9 persons with a
disease) or more extreme results assuming the null hypothesis is true.
If this probability is sufficiently low (below our chosen significance
level), we reject the null hypothesis in favor of the alternative
hypothesis.</p>
<p>The binomial test result (~0.012) reveals that the prevalence of the
disease among the individuals exposed to air pollution was indeed
significantly different from that of the population.</p>
</div>
<div class="section level2">
<h2 id="what-if-we-did-many-similar-experiments">What if we did many similar experiments?<a class="anchor" aria-label="anchor" href="#what-if-we-did-many-similar-experiments"></a>
</h2>
<p>Conducting a single study might not provide conclusive evidence due
to various factors such as sample variability, random chance, and other
unknown influences.</p>
<p>We decide to investigate the potential impact of air pollution on
disease prevalence in 200 various locations. We want to assess whether
there is a significant difference in disease rates between groups
exposed to air pollution and the average for the whole population.</p>
<p>We want to know how hypothesis testing works, when we perform it
simultaneously or in quick succession on many tests.</p>
<p>We therefore decide to simulate the scenario where we conduct 200
tests, each with a 5% chance (alpha = 0.05) of producing a significant
result (i.e., a p-value less than 0.05) even when the null hypothesis is
true.</p>
<p>Our null hypothesis in each location is that there is no real
difference in disease rates between the groups exposed to air pollution
and the average for the whole population.</p>
<figure><img src="../fig/Scenario_100%20individuals%20get%20tested%20for%20a%20disease.%20The%20disease%20prevalence%20is%200.04.%20The%20experiment%20is%20repeated%20200%20times.png" alt="Figure_2: A Scenario where 100 individuals get tested for a disease. The disease prevalence is 0.04. The experiment is repeated 200 times" class="figure mx-auto d-block"><div class="figcaption">Figure_2: A Scenario where 100 individuals get
tested for a disease. The disease prevalence is 0.04. The experiment is
repeated 200 times</div>
</figure><p>To do this, we write a program in R, which simulates study results
when the prevalence in the test group is 4% (null hypothesis is true).
We run these experiments to see what would happen if we kept doing tests
even when there was not actually any difference.</p>
<figure><img src="../fig/What-is-multiple-testing-rendered-Simulating%20200%20test%20groups-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>We are interested in determining if the observed proportion
significantly deviates from the expected proportion (0.4) in either
direction (either higher or lower). As this is a two-tailed binomial
test, we needed to adjust the bounds for what is considered
“significant” based on the significance level of 0.05. We are interested
in the extreme tails of the distribution that contain 2.5% of the data
on each side.</p>
<p>We therefore use <code>qbinom()</code> function in this simulation to
calculate the quantiles of the binomial distribution corresponding to
the 2.5th and 97.5th percentiles. This gives us the bounds that would
contain 95% of the data under the null hypothesis. We then classify a
group as “significant” if the observed number of patients falls below
the 2.5th percentile or above the 97.5th percentile of the binomial
distribution.</p>
<p>In the resulting histogram, we find that even in a world where there
was no true difference in disease prevalence, about 5% of our simulated
experiments yielded statistically significant results purely by chance
(the red bars).</p>
<p>It is important to note that the significance level (α) that we
choose for each individual test directly impacts the rate of false
positives. This is basically the <strong>Comparison-Wise Error Rate
(CWER)</strong>, the probability of making a Type I error (false
positive) in a single hypothesis test. In our example, we have set
α=0.05 for each individual test, which means we are essentially saying
that we are willing to accept a 5% chance of making a false positive
error for each test, and this means that for 100 tests, we expect about
5 false positives.</p>
<p>By running this simulation multiple times, we can observe how often
we get false positive results when there should be none. This helps us
understand the likelihood of obtaining a significant result purely by
chance, even if there is no true effect or difference.</p>
<div id="discussion1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#discussion1"></a>
</h3>
<div class="callout-content">
<ul>
<li>If we set α=0.01 for each individual test, which means we are
essentially saying that we are willing to accept a 1% chance of making a
false positive error for each test,what is the number of false positives
we should expect for 100 tests?</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">

</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="key-points">Key points<a class="anchor" aria-label="anchor" href="#key-points"></a>
</h3>
<ul>
<li>Through this exercise, we learn a valuable lesson about the dangers
of multiple testing.</li>
<li>We realize that without proper adjustments, the likelihood of
encountering false positives (rejecting a true null hypothesis) increase
with each additional comparison.</li>
<li>We need some additional control to make sure we found more hits than
expected by chance.</li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="so-what-is-multiple-testing">So what is multiple testing?<a class="anchor" aria-label="anchor" href="#so-what-is-multiple-testing"></a>
</h2>
<p>Multiple testing refers to the practice of conducting numerous
hypothesis tests simultaneously or repeatedly on the same data set. It
is typically motivated by the desire to explore different aspects of the
data or to investigate multiple hypotheses. Researchers employ multiple
tests to examine various relationships, comparisons, or associations
within their dataset, such as comparing means, proportions,
correlations, or other statistical analyses that involve hypothesis
testing.</p>
<div id="accordionInstructor1" class="accordion instructor-note accordion-flush">
<div class="accordion-item">
<button class="accordion-button instructor-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseInstructor1" aria-expanded="false" aria-controls="collapseInstructor1">
  <h3 class="accordion-header" id="headingInstructor1">  <div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="edit-2"></i></div> Instructor Note </h3>
</button>
<div id="collapseInstructor1" class="accordion-collapse collapse" aria-labelledby="headingInstructor1" data-bs-parent="#accordionInstructor1">
<div class="accordion-body">
<p>Inline instructor notes can help inform instructors of timing
challenges associated with the lessons. They appear in the “Instructor
View”</p>
</div>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</div></section><section id="aio-Types-of-errors"><p>Content from <a href="Types-of-errors.html">Types of errors and error rates</a></p>
<hr>
<p>Last updated on 2024-07-23 |

        <a href="https://example.com/FIXME-template-source/edit/main/episodes/Types-of-errors.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 10 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are false positives and false negatives and how do they
manifest in a confusion matrix?</li>
<li>What are some of real examples where false positives and false
negatives have different implications and consequences?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand the concept of false positives and false negatives and
how they are represented in a confusion matrix</li>
<li>Analyse and discuss scnarios where false positives and false
negatives pose distinct challenges and implications</li>
<li>Highlight situations where minimizing each type of error is
crucial</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="types-of-errors">Types of errors<a class="anchor" aria-label="anchor" href="#types-of-errors"></a>
</h1>
<p>In hypothesis testing, the evaluation of statistical significance
involves making decisions based on sample data. However, these decisions
are not without errors. In this tutorial, we will explore the concept of
errors in hypothesis testing (Type I and Type II errors), and their
implications.</p>
<div class="section level2">
<h2 id="type-i-error">Type I Error<a class="anchor" aria-label="anchor" href="#type-i-error"></a>
</h2>
<p>Type I error occurs when we reject a null hypothesis that is actually
true. For example, in our previous example where we conducted many
similar experiments, we experienced type I error by concluding that
exposure to air pollution has an effect on disease prevalence (rejecting
the null hypothesis) when it actually has no effect. Type I errors
represent false positives and can lead to incorrect conclusions,
potentially resulting in wasted resources or misguided decisions.</p>
</div>
<div class="section level2">
<h2 id="type-ii-error">Type II Error<a class="anchor" aria-label="anchor" href="#type-ii-error"></a>
</h2>
<p>Type II error occurs when we fail to reject a null hypothesis that is
actually false. For example, when we fail to conclude that exposure to
air pollution has an effect on disease prevalence (failing to reject the
null hypothesis) when it actually has a positive effect. Type II errors
represent false negatives and can result in missed opportunities or
overlooking significant effects.</p>
<p>One common way when this error occurs in the context of hypothesis
testing, is when a method with low <strong>statistical power</strong> is
chosen (if the sample size is small or the effect size (difference in
disease prevalence) is small). Statistical power refers to the
probability of correctly rejecting the null hypothesis when it is indeed
false. In simpler terms, it measures the likelihood of detecting a true
effect or difference if it exists. A test with high power is more likely
to detect a real effect, while a test with low power is more likely to
miss detecting a real effect, leading to a Type II error (false
negative).</p>
</div>
<div class="section level2">
<h2 id="confusion-matrix">Confusion Matrix<a class="anchor" aria-label="anchor" href="#confusion-matrix"></a>
</h2>
<p>In the context of hypothesis testing, we can conceptualize Type I and
Type II errors using a confusion matrix. The confusion matrix represents
the outcomes of hypothesis testing as True Positives (correctly
rejecting H0), False Positives (incorrectly rejecting H0), True
Negatives (correctly failing to reject H0), and False Negatives
(incorrectly failing to reject H0).</p>
<figure><img src="../fig/Figure%201%20Errors%20in%20hypothesis%20testing.png" alt="Figure_4: Errors in hypothesis testing and how they arise" class="figure mx-auto d-block"><div class="figcaption">Figure_4: Errors in hypothesis testing and how
they arise</div>
</figure>
</div>
</div>
<div class="section level1">
<h1 id="the-problem">The problem<a class="anchor" aria-label="anchor" href="#the-problem"></a>
</h1>
<p>In hypothesis testing, the occurrence of Type I and Type II errors
can have different implications depending on the context of the problem
being addressed. It is crucial to understand which errors are
problematic in which situation to be able to make informed decisions and
draw accurate conclusions from statistical analyses.</p>
<p>Type I errors, are particularly problematic in situations where the
cost or consequences of incorrectly rejecting a true null hypothesis are
high. Again if we refer back to our example, if we incorrectly conclude
that there is a significant difference in disease rates between the test
groups exposed to air pollution and the average for the whole population
when, in fact, there is no such difference, it could lead to misguided
policies or interventions targeting air pollution reduction. For
instance, authorities might implement costly environmental regulations
or public health measures based on erroneous conclusions. In this case,
the <strong>consequences</strong> include misallocation of resources,
leading to unnecessary financial burdens or societal disruptions.
Moreover, public trust in scientific findings and policy decisions may
be eroded if false positives lead to ineffective or burdensome
interventions.</p>
<p>Type II errors, are problematic when failing to detect a significant
effect has substantial consequences.If we fail to detect a significant
difference in disease rates between the test group and the population
average when there actually is a difference due to air pollution
exposure, it could result in overlooking a serious public health
concern. In this case, individuals living in polluted areas may continue
to suffer adverse health effects without receiving appropriate attention
or interventions. The <strong>consequences</strong> include increased
morbidity and mortality rates among populations exposed to high levels
of air pollution. Additionally, delayed or inadequate response to
environmental health risks may exacerbate inequalities in health
outcomes.</p>
<div class="section level2">
<h2 id="the-example-of-cancer-screening">The example of cancer screening<a class="anchor" aria-label="anchor" href="#the-example-of-cancer-screening"></a>
</h2>
<p>Cancer screening exemplifies a medical testing paradox, where the
interpretation of test results can be influenced by factors such as
disease prevalence, test sensitivity, and specificity.</p>
<p>Let us say that in a sample of 1000 women, 1% (10) have cancer, while
the remaining 99% (990) do not have cancer. This gives us the prevalence
of a disease. However, after testing, the test results show that out of
the 10 women with cancer, 9 receive a <strong>true positive</strong>
result (correctly identified as positive), and 1 receives a
<strong>false negative</strong> result (incorrectly identified as
negative). False negatives can delay the diagnosis and treatment of
cancer, allowing the disease to progress unchecked and potentially
reducing the effectiveness of treatment options. This can result in
poorer outcomes and decreased survival rates for patients. In addition,
out of the 990 women without cancer, 89 receive <strong>false
positive</strong> results (incorrectly identified as positive), and 901
receive <strong>true negative</strong> results (correctly identified as
negative). False positive can lead to unnecessary follow-up tests,
procedures, and treatments for individuals who do not have cancer. It
can cause anxiety, physical discomfort, and financial burden for
patients, as well as strain on healthcare resources.</p>
<figure><img src="../fig/cancer-paradox.png" alt="A tree diagram describing the outcomes of a breast cancer test" class="figure mx-auto d-block"><div class="figcaption">A tree diagram describing the outcomes of a
breast cancer test</div>
</figure><p>We could interpret that:</p>
<ul>
<li><p>The <strong>probability</strong> that a woman who receives a
positive result actually has cancer is ≈ 1/10 ( 9/ (9 + 89)). This is
the positive predictive value (PPV) of the test.</p></li>
<li><p>The <strong>sensitivity</strong> of the test, which measures its
ability to detect the presence of disease is 90% (9/10 * 100). This
means that the false negative rate of the test is 10%.</p></li>
<li><p>The <strong>specificity</strong> of the test, which measures its
ability to correctly identify individuals without the disease is ≈ 91%
(901/990 * 100). Here, the false positive rate of the test is
9%.</p></li>
</ul>
<p>While the test may have high accuracy in terms of sensitivity and
specificity, the positive predictive value is relatively low due to the
low prevalence of the disease in the population. This means that a
positive result from the test does not strongly predict the presence of
the disease in an individual. Similarly, false positives and false
negatives can affect the negative predictive value of the test, which
measures its ability to correctly identify individuals who do not have
cancer. False negatives decrease the negative predictive value, while
false positives increase it, potentially leading to misinterpretation of
test results.</p>
<p>This example underscores the complexity of interpreting medical test
results and emphasizes the need to consider factors such as disease
prevalence, test sensitivity, and specificity in clinical
decision-making. Increasing sensitivity may reduce false negatives but
can also increase false positives, and vice versa. Thus, optimizing the
trade-off between sensitivity and specificity is crucial to minimize
false positives and false negatives while maximizing the accuracy of the
screening test.</p>
<div class="section level3">
<h3 id="what-do-we-learn">What do we learn?<a class="anchor" aria-label="anchor" href="#what-do-we-learn"></a>
</h3>
<p>In many real-world scenarios, there is a trade-off between Type I and
Type II errors, and the appropriate balance depends on the specific
goals and constraints of the problem. Reseachers may prioritize one over
the other based on the severity of the consequences. For example, in
cancer screenings, minimizing false negatives (Type II errors) is
typically prioritized to avoid missing potential cases of cancer, even
if it leads to an increase in false positives (Type I errors).</p>
<p>Effective evaluation of Type I and Type II errors necessitates a
comprehensive consideration of the associated costs, risks, and ethical
implications. This holistic approach enhances the validity and
reliability of research findings by ensuring that decisions regarding
hypothesis acceptance or rejection are informed not only by statistical
significance but also by the potential real-world consequences of both
false positives and false negatives. By carefully weighing these
factors, researchers can make informed decisions that minimize the
likelihood of erroneous conclusions while maximizing the practical
relevance and ethical soundness of their findings.</p>
</div>
</div>
<div class="section level2">
<h2 id="error-rates">Error rates<a class="anchor" aria-label="anchor" href="#error-rates"></a>
</h2>
<p>In the following chapters, we’ll see different methods that all aim
to control for false positives in different settings.</p>
<p>The main distinction between them lies in the <em>error rate</em>
that they control for. We’ll get to know three of them:</p>
<ul>
<li>The comparison-wise error rate, where you use the p-value as you
obtain it from each individual test (no correction).</li>
<li>The family-wise error rate, where we like to avoid <em>any</em>
false positives.</li>
<li>The false-discovery rate, where we like to control the
<em>fraction</em> of false positives within our hits.</li>
</ul>
<p>In terms of settings, we’ll see screening scenarios and multiple
comparisons, and in both settings, different methods exist to control
for the different error rates.</p>
<p>The plethora of methods can often be overwhelming, therefore it’s
good to keep in mind that most of the time in practice, the workflow can
be broken down into three simple steps:</p>
<ol style="list-style-type: decimal">
<li>Clearly define your <strong>research question</strong>, and describe
the testing scenario.<br>
</li>
<li>Based on your research question, choose an appropriate <strong>error
rate</strong> that you need to control for.</li>
<li>Choose a <strong>method</strong> that controls for the selected
error rate that applies for the testing scenario at hand.</li>
</ol>
<p>Choosing the error rate is often a philosophical question. Before we
dive into the details of the individual methods, let’s get an overview
on the three error rates we’re considering. Which one to choose depends
on how the hypothesis tests in your scenario are connected.</p>
<figure><img src="../fig/error-rates.png" alt="Which error rate should you control for?" class="figure mx-auto d-block"><div class="figcaption">Which error rate should you control for?</div>
</figure><ul>
<li>If each of these tests answers a separate research question, you can
apply the comparison-wise error rate, which means you don’t have to
correct anything.</li>
<li>Sometimes, one incorrect rejection changes the overall conclusion in
your research setting. For example, if you decide on the safety of a
chemical, which is a mixture of ingredients. You can ask for each
ingredient whether it’s safe (null hypothesis: it’s safe). If you all
null hypohteses are true (all ingredients are safe), you’d like to
conclude that the entire chemical is safe. However, if you falsely
reject one individual null hypothesis, this will change the overall
conclusion (you falsely call the chemical unsafe). Therefore, you’d like
to control the probability of making any false rejections, which is
called the family-wise error rate.</li>
<li>In many scenarios, a few false positives won’t change the overall
conclusion. For example, in biological screens, we aim for a hit list of
candidate genes/proteins/substances that play a role in a process of
interest. We can then further verify candidates, or identify pathways
that are over-represented in the hits. For this, we can live with a few
false positives, but we’d like to make sure that <em>most of</em> what
we call a hit, actually <em>is</em> a hit. For such scenarios, we
control for the <em>false discovery rate</em>: the percentage of false
positives among the hits.</li>
</ul>
<div id="accordionInstructor1" class="accordion instructor-note accordion-flush">
<div class="accordion-item">
<button class="accordion-button instructor-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseInstructor1" aria-expanded="false" aria-controls="collapseInstructor1">
  <h3 class="accordion-header" id="headingInstructor1">  <div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="edit-2"></i></div> Instructor Note </h3>
</button>
<div id="collapseInstructor1" class="accordion-collapse collapse" data-bs-parent="#accordionInstructor1" aria-labelledby="headingInstructor1">
<div class="accordion-body">
<p>Inline instructor notes can help inform instructors of timing
challenges associated with the lessons. They appear in the “Instructor
View”</p>
</div>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</div></section><section id="aio-Family-wise-error-rate"><p>Content from <a href="Family-wise-error-rate.html">Family-wise error rate</a></p>
<hr>
<p>Last updated on 2024-07-23 |

        <a href="https://example.com/FIXME-template-source/edit/main/episodes/Family-wise-error-rate.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is the family-wise error rate (FWER), and why is it important
in multiple testing scenarios?</li>
<li>How does the Bonferroni procedure adjust p-values to control the
FWER, and what are its limitations?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand the concept of the family-wise error rate (FWER) and its
significance in multiple testing, including the implications of making
multiple comparisons without controlling for FWER.</li>
<li>Learn the Bonferroni procedure for adjusting p-values to maintain
the FWER at a specified level, and recognize when alternative methods
may be more appropriate or effective in controlling for multiple
comparisons.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>In statistical hypothesis testing, conducting multiple tests
simultaneously increases the likelihood of making at least one
false-positive error. In this episode, we will explore the family-wise
error rate (FWER), and discuss methods to account for multiple
comparisons, using practical examples.</p>
<section><h2 class="section-heading" id="a-multi-hypothesis-testing-framework">A Multi-Hypothesis Testing Framework<a class="anchor" aria-label="anchor" href="#a-multi-hypothesis-testing-framework"></a>
<a class="anchor" aria-label="anchor" href="#a-multi-hypothesis-testing-framework"></a>
</h2>
<hr class="half-width">
<p>In multiple testing scenarios, we often have an overarching
hypothesis that encompasses several individual hypotheses, each
examining specific aspects or relationships within the data. This
approach allows us to explore various facets of the research question
comprehensively.</p>
<p>Going back to our study investigating the effects of air pollution on
the prevalence of a disease, the overarching hypothesis could be
formulated as follows:</p>
<p><strong>Exposure to air pollution is associated with increased
prevalence of the disease</strong></p>
<p>Under this overarching hypothesis, several individual hypotheses can
be formulated to examine different aspects of the relationship between
air pollution exposure and disease prevalence. These individual
hypotheses may focus on various pollutants, different health outcomes,
or specific populations.</p>
<p>An example using three individual null hypotheses:</p>
<p><strong><span class="math inline">\(H_{0,1}\)</span></strong>:
Exposure to air pollution is not associated with increased disease
prevalence in region 1.</p>
<p><strong><span class="math inline">\(H_{0,2}\)</span></strong>:
Exposure to nitrogen dioxide is not associated with increased disease
prevalence.</p>
<p><strong><span class="math inline">\(H_{0,3}\)</span></strong>:
Long-term exposure to ozone (O3) is not associated with an increased
prevalence of the disease.</p>
<p>The three null hypotheses can be combined to the following overall
null hypothesis:</p>
<p><strong><span class="math inline">\(H_{0}\)</span></strong>: Air
pollution is not associated with an increased prevalence of the
disease.</p>
<div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>As soon as one of the individual null hypotheses is rejected, we also
reject the overall null hypothesis. Rejecting the overall null
hypothesis means that at least one of the individual null hypotheses is
false. Therefore, we will want to make sure that we have not a single
false positive among our individual hypothesis tests.</p>
</div>
</div>
</div>
<figure><img src="../fig/overarching-hypothesis.png" alt="Relationship between Overall Hypothesis and Individual Hypotheses (Effects of Air Pollution on Disease Prevalence)" class="figure mx-auto d-block"><div class="figcaption">Relationship between Overall Hypothesis and
Individual Hypotheses (Effects of Air Pollution on Disease
Prevalence)</div>
</figure><p>In this illustration, each individual hypothesis delves into a
distinct facet of the overarching research inquiry, enabling us to
thoroughly examine the intricate connection between air pollution
exposure and disease prevalence. By scrutinizing the impacts of various
air pollutants on the disease, this method encourages a systematic
exploration of diverse factors and aids in revealing potential
associations or patterns within the data set.</p>
<p>Now, let us assume that after data collection, for hypothesis 1, we
find that 15 out of 100 individuals exposed to high levels of
particulate matter develop the disease, for hypothesis 2, 20 out of 100
individuals exposed to high levels of nitrogen dioxide develop the
disease and for hypothesis 3, 5 out of 100 individuals exposed to high
levels of ozone develop the disease.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span> <span class="op">=</span> <span class="fl">100</span> <span class="co"># number of test persons</span></span>
<span><span class="va">p</span> <span class="op">=</span> <span class="fl">0.04</span> <span class="co"># Known prevalence of the disease in the general population</span></span>
<span></span>
<span><span class="va">individuals_suffered</span> <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="fl">15</span>, <span class="fl">9</span>, <span class="fl">5</span><span class="op">)</span> <span class="co"># number of individuals who suffered from the disease for each hypothesis</span></span></code></pre>
</div>
<p>We can visualise this as follows:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">complement</span> <span class="op">&lt;-</span> <span class="va">n</span> <span class="op">-</span> <span class="va">individuals_suffered</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu">rbind</span><span class="op">(</span><span class="va">individuals_suffered</span>, <span class="va">complement</span><span class="op">)</span></span>
<span><span class="fu">colnames</span><span class="op">(</span><span class="va">data</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu">c</span><span class="op">(</span><span class="st">"particles"</span>,<span class="st">"nitrogene"</span>, <span class="st">"O3"</span><span class="op">)</span></span>
<span><span class="fu">rownames</span><span class="op">(</span><span class="va">data</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu">c</span><span class="op">(</span><span class="st">"disease"</span>,<span class="st">"healthy"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">mosaicplot</span><span class="op">(</span><span class="fu">t</span><span class="op">(</span><span class="va">data</span><span class="op">)</span>,main<span class="op">=</span><span class="st">""</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="../fig/Family-wise-error-rate-rendered-unnamed-chunk-3-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Let us assume we conduct statistical tests for each of these
hypotheses, resulting in p-values for each test. For simplicity, let us
maintain our significance level at <span class="math inline">\(\alpha=0.05\)</span> for each individual test. We
can conduct binomial tests for each hypothesis and calculate the
p-values in R.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Calculate the p-values for each hypothesis using the binomial probability mass function</span></span>
<span><span class="va">p_values</span> <span class="op">=</span> <span class="fu">sapply</span><span class="op">(</span><span class="va">individuals_suffered</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">p_value</span> <span class="op">=</span> <span class="fu">binom.test</span><span class="op">(</span><span class="va">x</span>,n<span class="op">=</span><span class="va">n</span>,p<span class="op">=</span><span class="va">p</span><span class="op">)</span><span class="op">$</span><span class="va">p.value</span></span>
<span>  <span class="kw">return</span><span class="op">(</span><span class="va">p_value</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu">length</span><span class="op">(</span><span class="va">p_values</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu">cat</span><span class="op">(</span><span class="fu">sprintf</span><span class="op">(</span><span class="st">"Hypothesis %d: p = %.4f\n"</span>, <span class="va">i</span>, <span class="va">p_values</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="co"># Print the p-values for each hypothesis</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Hypothesis 1: p = 0.0000
Hypothesis 2: p = 0.0190
Hypothesis 3: p = 0.6033</code></pre>
</div>
</section><section><h2 class="section-heading" id="the-probability-of-having-at-least-one-false-positive-among-m-tests-fwer">The probability of having at least one false positive among <span class="math inline">\(m\)</span> tests (FWER)<a class="anchor" aria-label="anchor" href="#the-probability-of-having-at-least-one-false-positive-among-m-tests-fwer"></a>
<a class="anchor" aria-label="anchor" href="#the-probability-of-having-at-least-one-false-positive-among-m-tests-fwer"></a>
</h2>
<hr class="half-width">
<p>Let us assume each test has a probability <span class="math inline">\(\alpha\)</span> of producing a false positive, and
that we have <span class="math inline">\(m\)</span> independent
tests.</p>
<p>The probability that a single test does not produce a false positive
is calculated as: <span class="math display">\[1-\alpha\]</span></p>
<p>Since the tests are independent, the probability that none of the
<span class="math inline">\(m\)</span> tests produces a false positive
is calculated as: <span class="math display">\[(1-\alpha)^m\]</span></p>
<p>Therefore, the probability of at least one false positive is the
complement of the probability that none of the tests produce a false
positive, and is calculated as: <span class="math display">\[P(\text{at
least one false positive})=1−(1−\alpha)^m\]</span></p>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge1"></a>
</h3>
<div class="callout-content">
<ol style="list-style-type: decimal">
<li>In the above example, we have three different tests. Will the
probability of making one false positive still be 0.05?</li>
<li>What can we do to decrease the probability of any false
positive?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li>We can use R to calculate the probability of at least one false
positive for our example with three tests:</li>
</ol>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m</span> <span class="op">&lt;-</span> <span class="fl">3</span></span>
<span><span class="va">alpha</span> <span class="op">&lt;-</span> <span class="fl">0.05</span></span>
<span></span>
<span><span class="fl">1</span> <span class="op">-</span> <span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">alpha</span><span class="op">)</span><span class="op">^</span><span class="va">m</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 0.142625</code></pre>
</div>
<ol start="2" style="list-style-type: decimal">
<li>By decreasing the alpha.</li>
</ol>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="the-bonferroni-correction">The Bonferroni correction<a class="anchor" aria-label="anchor" href="#the-bonferroni-correction"></a>
<a class="anchor" aria-label="anchor" href="#the-bonferroni-correction"></a>
</h2>
<hr class="half-width">
<div class="section level3">
<h3 id="adjusting-the-significance-level">Adjusting the significance level<a class="anchor" aria-label="anchor" href="#adjusting-the-significance-level"></a>
</h3>
<p>Now, to calculate the probability of having any false positive within
the set of tests (also known as family-wise error rate or FWER), we can
use methods such as the Bonferroni correction. This method adjust the
significance level for each test to control for multiple testing.</p>
<p>The Bonferroni procedure adjusts the significance level for each
individual test by dividing the desired overall significance level by
the number of tests conducted (m).</p>
<p><span class="math display">\[\alpha_{\text{Bonf}}= \alpha/m
\]</span></p>
<p>Where:</p>
<p>α is the desired overall significance level (usually set to 0.05). m
is the number of hypothesis tests conducted.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">FWER</span> <span class="op">&lt;-</span> <span class="fl">0.05</span><span class="co"># Define the desired Family-wise error rate</span></span>
<span></span>
<span><span class="va">m</span> <span class="op">&lt;-</span> <span class="fu">length</span><span class="op">(</span><span class="va">p_values</span><span class="op">)</span><span class="co"># Calculate the number of hypothesis tests conducted (m)</span></span>
<span></span>
<span><span class="va">alpha_bonf</span> <span class="op">&lt;-</span> <span class="va">FWER</span> <span class="op">/</span> <span class="va">m</span> <span class="co"># Calculate Bonferroni adjusted significance level</span></span>
<span></span>
<span><span class="va">alpha_bonf</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 0.01666667</code></pre>
</div>
<p>Since in our example above we have three tests, the Bonferroni
corrected significance level is <span class="math inline">\(\alpha_{\text{bonf}} = 0.05/3 \approx
0.0167\)</span>.</p>
<p>In this lesson, we’ll not go into the proof for why this method
works. But we can convince ourselves that it does, by calculating the
FWER for with the adjusted significance level, using the formula derived
above.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fl">1</span> <span class="op">-</span> <span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="fl">0.0167</span><span class="op">)</span><span class="op">^</span><span class="fl">3</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 0.04926799</code></pre>
</div>
<p>With the adjusted significance level of <span class="math inline">\(\alpha \approx 0.0167\)</span> for three tests, we
are back to the desired <span class="math inline">\(FWER\)</span> of
5%.</p>
<div class="section level4">
<h4 id="in-our-example">In our example<a class="anchor" aria-label="anchor" href="#in-our-example"></a>
</h4>
<p>For each individual test, we compare the calculated p-value to this
adjusted significance level. If the p-value is less than or equal to
0.0167, we reject the null hypothesis for that test.</p>
<p>Based on the Bonferroni correction, we reject the null hypothesis for
Hypotheses 1, indicating significant associations between particulate
matter with disease prevalence. However, for Hypothesis 2 (nitrogen
dioxide exposure) and 3 (ozone exposure), we fail to reject the null
hypothesis, suggesting no significant association with disease
prevalence at the adjusted significance level. This adjustment for
multiple testing helps control the overall probability of making at
least one false-positive error across all tests conducted.</p>
<p>In this example, while the evidence supports associations between
certain air pollutants and disease prevalence, it does not provide
conclusive evidence to reject the overarching null hypothesis entirely.
Instead, it suggests a nuanced interpretation wherein the relationship
between air pollution exposure and disease prevalence may vary depending
on the specific pollutant considered. Therefore, further investigation
and analysis may be necessary to fully elucidate the relationship
between air pollution exposure and disease prevalence and to refine the
overarching null hypothesis accordingly. This could involve exploring
additional factors, conducting more comprehensive analyses, or
considering alternative statistical approaches to account for potential
confounding variables or sources of variability in the data.</p>
</div>
</div>
<div class="section level3">
<h3 id="adjusting-the-p-value">Adjusting the p-value<a class="anchor" aria-label="anchor" href="#adjusting-the-p-value"></a>
</h3>
<p>Instead of changing the significance level, another (equivalent)
calculation is adjusting the p-values obtained from individual
hypothesis tests, followed by comparing the adjusted p-values to the
desired FWER. With this procedure, we can reject those individual null
hypotheses that have a p-value below the desired FWER (<span class="math inline">\(p &lt; \text{FWER}\)</span>).</p>
<p><span class="math display">\[p_{\text{Bonf}}= p \times m\]</span></p>
<div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge2"></a>
</h3>
<div class="callout-content">
<ol style="list-style-type: decimal">
<li><p>Look up <code>p.adjust</code>. This function adjusts P-values for
Multiple Comparisons.</p></li>
<li><p>Using our previous example, can you adjust the p-values obtained
from individual hypothesis tests?</p></li>
<li><p>Which of the individual hypotheses can will be rejected at a
FWER=0.05?</p></li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li>You can look up the function as follows:</li>
</ol>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">help</span><span class="op">(</span><span class="st">"p.adjust"</span><span class="op">)</span></span></code></pre>
</div>
<p>2.In R, one can adjust the p-values as follows:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">p.adjust</span><span class="op">(</span><span class="va">p_values</span>, method <span class="op">=</span> <span class="st">"bonferroni"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 3.244612e-05 5.697576e-02 1.000000e+00</code></pre>
</div>
<ol start="3" style="list-style-type: decimal">
<li>We can check which of the individual hypotheses will be rejected at
a FWER=0.05:</li>
</ol>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">p.adjust</span><span class="op">(</span><span class="va">p_values</span>, method <span class="op">=</span> <span class="st">"bonferroni"</span><span class="op">)</span> <span class="op">&lt;</span> <span class="fl">0.05</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1]  TRUE FALSE FALSE</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>The conclusion is the same as when changing the significance level
for each test.</p>
</div>
</section><section><h2 class="section-heading" id="family-wise-error-rate-and-power">Family-wise error rate and power<a class="anchor" aria-label="anchor" href="#family-wise-error-rate-and-power"></a>
<a class="anchor" aria-label="anchor" href="#family-wise-error-rate-and-power"></a>
</h2>
<hr class="half-width">
<div class="section level3">
<h3 id="what-is-statistical-power">What is statistical power?<a class="anchor" aria-label="anchor" href="#what-is-statistical-power"></a>
</h3>
<p>Statistical power is the probability of detecting an effect with a
statistical test, given that the effect really exists. Expressing this
with the terms that we learned in this tutorial, power is defined as</p>
<p><span class="math display">\[ P(\text{rejection} | \text{null
hypothesis is false}).\]</span></p>
<div id="challenge3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge3"></a>
</h3>
<div class="callout-content">
<p>Suppose we test a set of hypotheses. How does applying the Bonferroni
correction impact the power of these tests?</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<p>The Bonferroni correction decreases the power of the test, especially
when we have many tests.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="the-number-of-tests-matters">The number of tests matters<a class="anchor" aria-label="anchor" href="#the-number-of-tests-matters"></a>
</h3>
<p>Remember that the adjusted significance level depends on the number
of tests that we conduct:</p>
<p><span class="math display">\[\alpha_{\text{Bonf}}= \alpha/m
\]</span></p>
<p>Let’s look at some adjusted significance levels for different numbers
of tests, given that we like to control the family-wise error rate at
<span class="math inline">\(\alpha_\text{FWER} = 0.05\)</span>:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n_test</span> <span class="op">&lt;-</span> <span class="fu">seq</span><span class="op">(</span><span class="fl">1</span>,<span class="fl">20000</span><span class="op">)</span></span>
<span><span class="va">alpha_adjusted</span> <span class="op">&lt;-</span> <span class="fl">0.05</span><span class="op">/</span><span class="va">n_test</span></span>
<span></span>
<span><span class="fu">data.frame</span><span class="op">(</span><span class="va">n_test</span>,<span class="va">alpha_adjusted</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">n_test</span>, y<span class="op">=</span><span class="fu">log10</span><span class="op">(</span><span class="va">alpha_adjusted</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span><span class="op">)</span></span></code></pre>
</div>
<p><img src="../fig/Family-wise-error-rate-rendered-unnamed-chunk-10-1.png" style="display: block; margin: auto;" class="figure">
We see that the adjusted <span class="math inline">\(\alpha\)</span> is
dropping quickly. For <span class="math inline">\(20000\)</span> tests,
which is a reasonable number in genomic screens, alpha will be:</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">last</span><span class="op">(</span><span class="va">alpha_adjusted</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 2.5e-06</code></pre>
</div>
<p>We’ll only reject the null for tests that produce a p-value <span class="math inline">\(&lt;0.0000025\)</span>, which means the chances of
finding a hit are reduced drastically compared to before adjustment.</p>
<div id="accordionInstructor1" class="accordion instructor-note accordion-flush">
<div class="accordion-item">
<button class="accordion-button instructor-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseInstructor1" aria-expanded="false" aria-controls="collapseInstructor1">
  <h3 class="accordion-header" id="headingInstructor1">  <div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="edit-2"></i></div> Instructor Note </h3>
</button>
<div id="collapseInstructor1" class="accordion-collapse collapse" aria-labelledby="headingInstructor1" data-bs-parent="#accordionInstructor1">
<div class="accordion-body">
<p>Inline instructor notes can help inform instructors of timing
challenges associated with the lessons. They appear in the “Instructor
View”</p>
</div>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</section></section><section id="aio-False-discovery-rate"><p>Content from <a href="False-discovery-rate.html">False discovery rate</a></p>
<hr>
<p>Last updated on 2024-07-23 |

        <a href="https://example.com/FIXME-template-source/edit/main/episodes/False-discovery-rate.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How does correcting for the family-wise error rate (FWER) affect the
number of significant hits in large-scale data, such as RNA-Seq analysis
of 20,000 human genes?</li>
<li>What is the interpretation of a p-value histogram, and how can it be
used to assess the distribution of p-values in multiple testing
scenarios?</li>
<li>How can the Benjamini-Hochberg method be applied to control the
false discovery rate (FDR) in RNA-Seq data, and what are the benefits of
using this method over FWER correction?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Demonstrate how correcting for the family-wise error rate (FWER)
using methods like Bonferroni correction can lead to few or no
significant hits in large-scale testing scenarios.</li>
<li>Introduce the concept of p-value histograms, explain their
interpretation, and illustrate how they can be used to visualize the
distribution of p-values in multiple testing.</li>
<li>Explain the Benjamini-Hochberg method for controlling the false
discovery rate (FDR).</li>
<li>Provide practical examples and R code to apply the
Benjamini-Hochberg method to RNA-Seq data.</li>
<li>Discuss the advantages of controlling the FDR over FWER in the
context of large-scale genomic data.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h1>
<p>In high-throughput experiments like RNA-Seq, we often conduct
thousands of statistical tests simultaneously. This large number of
tests increases the risk of false positives. While controlling the
family-wise error rate (FWER), the probability of making at least one
type I error, is one approach to address false positives, it can be too
conservative, leading to few or no significant results. An alternative
approach is to control the <strong>False Discovery Rate (FDR)</strong>,
the expected proportion of false positives among the rejected
hypotheses, which offers a balance between identifying true positives
and limiting false positives. In this tutorial, we will learn how each
method affects the outcome.</p>
<div class="section level2">
<h2 id="example-the-airway-dataset-in-r">Example: The Airway dataset in R<a class="anchor" aria-label="anchor" href="#example-the-airway-dataset-in-r"></a>
</h2>
<p>The Airway dataset contains gene expression data from a study
investigating the effects of dexamethasone (a corticosteroid medication)
on airway smooth muscle cells. The dataset is part of the airway package
in Bioconductor, a project that provides tools for the analysis and
comprehension of high-throughput genomic data.</p>
<p>In differential expression analysis, thousands of statistical tests
are conducted: For each gene, one can test whether its expression is
different in cells with dexamethasone treatment, compareed to cells
without treatment. If the expression differs between the two condition,
we call the gene differentially expressed (DE). Like in the previous
example, we have a set up null hypotheses:</p>
<p><span class="math inline">\(H_{0,1}\)</span>: Gene 1 is not DE.</p>
<p><span class="math inline">\(H_{0,2}\)</span>: Gene 2 is not DE.</p>
<p>…</p>
<p><span class="math inline">\(H_{0,20000}\)</span>: Gene 20000 is not
DE.</p>
<p>Unlike in the air pollution example, our question is not whether
<em>any</em> of the genes is DE, but rather <em>which ones</em>: We’d
like to come up with a hit list of genes that can be further
investigated.</p>
<p>This is how the p-values are created:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="va">DESeq2</span><span class="op">)</span></span></code></pre>
</div>
<p>If you like to work with them without running the code, you can load
pre-computed p-values as follows:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">### HERE SHOULD BE CODE FOR LOADING THE DATA</span></span></code></pre>
</div>
<p>The table below shows the first six rows of the generated p-values
for each gene, the data which, we are going use to see how using FWER
and FDR to controlling for false positive differ.</p>
<table class="table">
<caption>Table 1: P_Values for each analysed gene</caption>
<thead><tr class="header">
<th align="left">gene</th>
<th align="right">pvalue</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">ENSG00000000003</td>
<td align="right">0.0286636</td>
</tr>
<tr class="even">
<td align="left">ENSG00000000419</td>
<td align="right">0.0428183</td>
</tr>
<tr class="odd">
<td align="left">ENSG00000000457</td>
<td align="right">0.7874802</td>
</tr>
<tr class="even">
<td align="left">ENSG00000000460</td>
<td align="right">0.6972820</td>
</tr>
<tr class="odd">
<td align="left">ENSG00000000938</td>
<td align="right">0.6215698</td>
</tr>
<tr class="even">
<td align="left">ENSG00000000971</td>
<td align="right">0.0885597</td>
</tr>
</tbody>
</table>
<p>20000 p-values are too many to list them all, but we can look at
their distribution by visualizing a p-value histogram. A p-value
histogram is a graphical representation that displays the distribution
of p-values obtained from multiple hypothesis tests. It can help us in
assessing the presence of true effects versus null effects and in
understanding the overall behavior of the tests. It can also help us to
better control for false positives. To understand how this works, we’ll
have to look into the theory. In the next section, we’ll learn</p>
<ul>
<li>that a p-value histogram is composed of two fractions: the null and
the alternative</li>
<li>what behavior we expect from the null fraction</li>
</ul>
<p>Understanding this will provide us with a tool for controlling the
False discovery rate.</p>
</div>
</div>
<div class="section level1">
<h1 id="the-theory-of-p-value-histograms">The Theory of P-value Histograms<a class="anchor" aria-label="anchor" href="#the-theory-of-p-value-histograms"></a>
</h1>
<p>To create a p-value histogram, we plot the p-values on the x-axis,
typically ranging from 0 to 1. The y-axis represents the frequency (or
count) of p-values falling within specific bins (intervals) of the
x-axis. Let’s do this for the <code>airway</code> p-values.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">gene_pvalues</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">pvalue</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>binwidth<span class="op">=</span><span class="fl">0.01</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning: Removed 51 rows containing non-finite values (`stat_bin()`).</code></pre>
</div>
<figure><img src="../fig/False-discovery-rate-rendered-unnamed-chunk-3-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><div class="section level2">
<h2 id="a-p-value-histogram-is-composed-of-two-fractions">A p-value histogram is composed of two fractions<a class="anchor" aria-label="anchor" href="#a-p-value-histogram-is-composed-of-two-fractions"></a>
</h2>
<p>We can think of the p-value histogram as being composed of two
fractions: the alternative and the null fraction.</p>
<p><img src="../fig/composition-of-pvalues.png" class="figure"> Together, the
alternative and null fraction give us a p-value histogram as the one
observed for the <code>airway</code> data.</p>
</div>
<div class="section level2">
<h2 id="why-are-these-our-expectations-of-the-null-and-alterative-fraction">Why are these our expectations of the null and alterative
fraction?<a class="anchor" aria-label="anchor" href="#why-are-these-our-expectations-of-the-null-and-alterative-fraction"></a>
</h2>
<p>Let’s start with the easier case: If we use a statistical test that
is good at detecting differentially expressed genes, then it will
produce low p-values for the DE genes, resulting in a peak close to 0.
Depending on the power of the test, the peak towards 0 is sharp (high
power, all the DE genes have low p-values) or flat and less pronounced
(low power, many DE genes have p-values &gt;&gt;0).</p>
<p>But why do we expect a uniform distribution of p-values that come
from genes where the null hypothesis is true? This comes from the
definition of p-values. We expect 5% of the p-values to be <span class="math inline">\(&lt;0.05\)</span>, 10% of the p-vales to be <span class="math inline">\(&lt;0.1\)</span>, etc. Unfortunately, for many
this definition is not intuitive. Therefore, we use a simulation of
tests where the null hypothesis is true, and use it to recap the
definition of p-values.</p>
<p>We learned that in the t-test, the test statistic <span class="math inline">\(t\)</span> follows a <em>t-distribution</em> under
the null hypothesis. So, when the null hypothesis is true, we expect the
value of <span class="math inline">\(t\)</span> to be randomly drawn
from a t-distribution. For demonstration purposes, we can simulate 2000
draws from a t-distribution (here, I choose the degrees of freedom to be
<span class="math inline">\(\nu = 5\)</span>, which is an arbitrary
choice) and visualize their distribution.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">set.seed</span><span class="op">(</span><span class="fl">55</span><span class="op">)</span></span>
<span><span class="va">ts</span> <span class="op">&lt;-</span> <span class="fu">rt</span><span class="op">(</span><span class="fl">2000</span>, df<span class="op">=</span><span class="fl">5</span><span class="op">)</span></span>
<span><span class="fu">data.frame</span><span class="op">(</span>t<span class="op">=</span><span class="va">ts</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">t</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>binwidth<span class="op">=</span><span class="fl">0.2</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>main<span class="op">=</span><span class="st">"Null distribution of t"</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="../fig/False-discovery-rate-rendered-unnamed-chunk-4-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>This is our null distribution!</p>
<p>Since we’ll decide significance based on the absolute value of t,
<span class="math inline">\(|t|\)</span>, I’ll calculate it here:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">abs_t</span> <span class="op">&lt;-</span> <span class="fu">abs</span><span class="op">(</span><span class="va">ts</span><span class="op">)</span><span class="co"># take their absolute values</span></span>
<span><span class="fu">data.frame</span><span class="op">(</span>abs_t<span class="op">=</span><span class="va">abs_t</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">abs_t</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>binwidth<span class="op">=</span><span class="fl">0.1</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>main<span class="op">=</span><span class="st">"Null distribution of |t|"</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="../fig/False-discovery-rate-rendered-unnamed-chunk-5-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>This is our null distribution of absolute t-values. In hypothesis
testing, we ask: “What is the probability in the null distribution to
observe a value at least as extreme as the observed <span class="math inline">\(|t|\)</span>?” So, what we’re looking at to answer
this question is the <em>cumulative distribution</em> of <span class="math inline">\(|t|\)</span>. While in practice, we’re looking at
the theoretical cumulative distribution, we’ll here look at the
cumulative distribution of our simulation, hence an <em>empirical</em>
cumulative distribution.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">data.frame</span><span class="op">(</span>abs_t <span class="op">=</span> <span class="va">abs_t</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">abs_t</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">stat_ecdf</span><span class="op">(</span><span class="op">)</span></span></code></pre>
</div>
<p><img src="../fig/False-discovery-rate-rendered-unnamed-chunk-6-1.png" style="display: block; margin: auto;" class="figure">
This cumulative distribution function answers the question “for a given
value of <span class="math inline">\(|t|\)</span>, how many other
elements of the simulation are <em>smaller than</em> this value?”. Which
is exactly the opposite of what we’re asking when calculating a p-value.
In fact, the p-value is defined as <span class="math inline">\(1-\text{CDF}(|t|)\)</span>, which looks like
this:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">data.frame</span><span class="op">(</span>abs_t <span class="op">=</span> <span class="va">abs_t</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">abs_t</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>y <span class="op">=</span> <span class="fl">1</span> <span class="op">-</span> <span class="va">..y..</span><span class="op">)</span>, stat<span class="op">=</span><span class="st">'ecdf'</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>y<span class="op">=</span><span class="st">"1-ECDF"</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="../fig/False-discovery-rate-rendered-unnamed-chunk-7-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Code adapted from <a href="https://stackoverflow.com/questions/37221303/how-to-plot-reverse-complementary-ecdf-using-ggplot" class="external-link">here</a>.
So when we calculate a p-value, we (visually speaking) look up the
observed value of <span class="math inline">\(|t|\)</span>
(<code>abs_t</code>) on the x-axis, and match it to the corresponding
1-ECDF on the y-axis, which is the resulting <span class="math inline">\(p\)</span>.</p>
<p>What does this tell us about the distribution of p-values? We could
go and slice the 1-ECDF into chunks of 5% of the data points:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">data.frame</span><span class="op">(</span>abs_t <span class="op">=</span> <span class="va">abs_t</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">abs_t</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>y <span class="op">=</span> <span class="fl">1</span> <span class="op">-</span> <span class="va">..y..</span><span class="op">)</span>, stat<span class="op">=</span><span class="st">'ecdf'</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>y<span class="op">=</span><span class="st">"1-ECDF"</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_hline</span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fu">seq</span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span>,by<span class="op">=</span><span class="fl">0.05</span><span class="op">)</span>,</span>
<span>             col<span class="op">=</span><span class="st">"gray"</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="../fig/False-discovery-rate-rendered-unnamed-chunk-8-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>The observations in each slice will make up one bin in the p-value
histogram which we’re going to create next.</p>
<p>All of these chunks should contain about 5% of the p-values -
<strong>by the definition of the p-value via the cumulative
distribution</strong>.</p>
<p>For instance, the upper 5% of the <span class="math inline">\(|t|\)</span>-values are be between <span class="math inline">\(2.6\)</span> and <span class="math inline">\(6.8\)</span>, so we give them p-values between
0.00 and 0.05. The next 5% of the <span class="math inline">\(|t|\)</span>-values will be between <span class="math inline">\(2.0\)</span> and <span class="math inline">\(2.6\)</span>, so we give them p-values between
0.05 and 0.10. And so on… resulting in the following p-value histogram
again sliced by 5%-bins for demonstration:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">data.frame</span><span class="op">(</span>abs_t <span class="op">=</span> <span class="va">abs_t</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>p <span class="op">=</span> <span class="fu">pt</span><span class="op">(</span><span class="va">abs_t</span>, df<span class="op">=</span><span class="fl">5</span>,lower.tail<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span><span class="op">*</span><span class="fl">2</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">p</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>binwidth<span class="op">=</span><span class="fl">0.05</span>, boundary<span class="op">=</span><span class="fl">0</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="../fig/False-discovery-rate-rendered-unnamed-chunk-10-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>We could also define the p-value via the empirical null
distribution:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">abs_t_vec</span> <span class="op">&lt;-</span> <span class="va">abs_t</span></span>
<span><span class="fu">data.frame</span><span class="op">(</span>abs_t <span class="op">=</span> <span class="va">abs_t</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">rowwise</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>p <span class="op">=</span> <span class="fu">mean</span><span class="op">(</span><span class="va">abs_t</span><span class="op">&gt;</span><span class="va">abs_t_vec</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">p</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>binwidth<span class="op">=</span><span class="fl">0.05</span>, boundary<span class="op">=</span><span class="fl">0</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="../fig/False-discovery-rate-rendered-unnamed-chunk-11-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Again, each 5% bin contains 5% (=100) of the p-values, because the
p-values are defined by the percentage of values of <span class="math inline">\(|t|\)</span> in the simulated null distribution
which are smaller than the observed one.</p>
<div class="section level3">
<h3 id="recap">Recap<a class="anchor" aria-label="anchor" href="#recap"></a>
</h3>
<ul>
<li>p-values are defined as via the cumulative distribution of the test
statistic under the null hypothesis</li>
<li>They answer the question: what percentage of values is expected more
extreme than the observed one?</li>
<li>If we bin our data into equally-sized quantiles, we expect to see
roughly the same number of p-values in each quantile.</li>
<li>Therefore: In a set of tests where the null hypothesis is true, and
where the test statistic behaves like expected in theory, we will see a
uniform distribution of p-values</li>
<li>This works for any test statistic, <em>as long as we know its
distribution</em>
</li>
</ul>
</div>
</div>
</div>
<div class="section level1">
<h1 id="the-false-discovery-rate">The False Discovery Rate<a class="anchor" aria-label="anchor" href="#the-false-discovery-rate"></a>
</h1>
<p>Let’s come back to our p-value histogram of differentially expressed
genes. We learned that it is composed of null and alternative fraction,
and we learned that in theory, the null fraction is uniformly
distributed.</p>
<p>We can use this knowledge to estimate the fraction of false positives
among the positives at an arbitrary p-value cut-off. This is illustrated
below.</p>
<figure><img src="../fig/p_value%20histogram%20decomposition.png" alt="A p-value histogram decomposition (adapted from MSMB)" class="figure mx-auto d-block"><div class="figcaption">A p-value histogram decomposition (adapted from
MSMB)</div>
</figure><p>Say, we determine some cut-off p-value, indicated by the red line.
Then we can visually estimate what percentage of tests right at this
threshold are false positives, namely by dividing the length of the
light red segment by the overall length of the line. That’s the local
<strong>fdr</strong>, and it applies to tests rejected just at this
threshold. Now we will probably reject all the tests below the
threshold, and to get the fraction of false positives within those, we
divide the dark gray area by the the total area to the left of the red
line. This is the capital letter <strong>FDR</strong> and it’s an
average property of all tests rejected below the threshold.</p>
<p>The false discovry rate is defined as <span class="math inline">\(FDR
= \frac{FP}{TP+FP}.\)</span></p>
<div class="section level2">
<h2 id="in-our-example">In our example<a class="anchor" aria-label="anchor" href="#in-our-example"></a>
</h2>
</div>
<div class="section level2">
<h2 id="adjusted-p-values">Adjusted p-values<a class="anchor" aria-label="anchor" href="#adjusted-p-values"></a>
</h2>
<p>At any given p-value cut-off, we can estimate the FDR. So could take
each p-values in our screen, use it as a cut-off and return the
estimated FDR. This is the adjusted p-value. It tells us the FDR that we
would have for our hits if we rejected the null for all genes with a
p-value equal to or lower than that for this gene. We can then decide to
call all tests a hit, for which the adjusted p-value is smaller than the
desired <span class="math inline">\(\alpha_\text{FWER}\)</span>.</p>
<div class="section level3">
<h3 id="trade-off-fp-and-fn">Trade-off FP and FN<a class="anchor" aria-label="anchor" href="#trade-off-fp-and-fn"></a>
</h3>
<p>If we set the threshold very low, we will have a low FDR (low
fraction of false positives), but will miss many of the differentially
expressed genes. When we move the red line to the right, that is
increase the threshold, we’ll capture more and more of the
differentially expressed genes, but at the cost of more and more false
positives, and if we go too far, we will almost exclusively admit more
False Positives.</p>
</div>
<div class="section level3">
<h3 id="more-on-p-value-histograms">More on p-value histograms<a class="anchor" aria-label="anchor" href="#more-on-p-value-histograms"></a>
</h3>
<p>If you encounter p-value histograms that don’t fit the expectation we
just discussed, have a look at this <a href="http://varianceexplained.org/statistics/interpreting-pvalue-histogram/" class="external-link">post
in varianceexplained.org</a></p>
</div>
</div>
<div class="section level2">
<h2 id="controlling-fdr-using-the-benjamini-hochberg-method">Controlling FDR Using the Benjamini-Hochberg Method<a class="anchor" aria-label="anchor" href="#controlling-fdr-using-the-benjamini-hochberg-method"></a>
</h2>
<p>The Benjamini-Hochberg (BH) method is a statistical procedure used to
control the FDR in multiple hypothesis testing, where the chance of
obtaining false positives increases. The BH method helps control the
proportion of false positives (false discoveries) among the rejected
hypotheses, thus providing a more balanced approach than traditional
methods like the <strong>Bonferroni correction</strong>, which can be
overly conservative. The BH procedure adjusts the p-values from multiple
tests to control the FDR. These adjusted p-values can be compared to a
significance threshold to determine which results are significant.</p>
<div class="section level3">
<h3 id="steps-of-the-benjamini-hochberg-procedure">Steps of the Benjamini-Hochberg Procedure<a class="anchor" aria-label="anchor" href="#steps-of-the-benjamini-hochberg-procedure"></a>
</h3>
<p>First the observed p-values are arranged in ascending order. Next,
ranks are assigned to the p-values, with the smallest p-value getting
rank 1, the next smallest rank 2, and so on. Then, for each p-value, the
BH critical value is calculated as;</p>
<p><span class="math display">\[\text{BH critical value} = \frac{i}{m}
\times Q\]</span></p>
<p>where, i is the rank, m is the total number of tests, and Q is the
desired FDR level (e.g., 0.05).</p>
<p>After this calculation, the largest p-value that is less than or
equal to its BH critical value is identified. This p-value and all
smaller p-values are considered significant. Optionally, one can adjust
the p-values to reflect the BH correction using software functions.</p>
<p>Now, let us continue with the generated p_values to apply the
Benjamini-Hochberg correction in r, and also plot the adjusted p values
for comparison.</p>
<p>What the Benjamini-Hochberg algorithm does, is that it estimates the
null component, and finds the threshold below which we should reject for
a desired FDR. Equivalently, and that’s the way it’s implemented in R,
we could say it produces adjusted p-values. And if we reject everything
below a certain adjusted p-value (say 5%), this will lead to an FDR of
5%, meaining that 5% of the hits are false-positives.</p>
</div>
</div>
</div>
<div class="section level1">
<h1 id="wrap-up">Wrap up<a class="anchor" aria-label="anchor" href="#wrap-up"></a>
</h1>
<p>We discussed that controlling the FDR trades some false positives for
the chances to discover more hits. Let’s compare the three error rates
that we encountered so far - comparison-wise error rate - family-wise
error rate - false discovery rate</p>
<p>in terms of the number of hits they return.</p>
<p>We can check how many genes were significantly differentially
expressed between the treated and untreated cells at α=0.05 before we
apply any type 1 error correction method:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>4655 is the number of significant differentially expressed genes before FWER correction</code></pre>
</div>
<p>Now, if we proceed to apply the Bonferroni method to correct the
family-wise error rate (FWER), we can then determine the number of genes
that show significant differential expression after the correction.</p>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Use <code>p.adjust</code> to calculate adjusted p-values using
Benjamini Hochberg. How many hits do you get if you control the FDR at
10%?</li>
<li>Do the same with Bonferroni</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">alpha</span><span class="op">=</span><span class="fl">0.1</span></span>
<span><span class="co"># Apply Benjamini-Hochberg correction</span></span>
<span><span class="va">p_adjusted</span> <span class="op">&lt;-</span> <span class="fu">p.adjust</span><span class="op">(</span><span class="va">gene_pvalues</span><span class="op">$</span><span class="va">pvalue</span>, method <span class="op">=</span> <span class="st">"BH"</span><span class="op">)</span></span>
<span><span class="va">significant_bh</span> <span class="op">&lt;-</span> <span class="va">p_adjusted</span> <span class="op">&lt;</span> <span class="va">alpha</span></span>
<span><span class="va">Benjamini_Hochberg_genes</span><span class="op">&lt;-</span><span class="fu">sum</span><span class="op">(</span><span class="va">significant_bh</span><span class="op">)</span> <span class="co"># Number of significant hits after Benjamini-Hochberg correction</span></span>
<span><span class="va">Benjamini_Hochberg_genes</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 2908</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">alpha</span><span class="op">=</span><span class="fl">0.1</span></span>
<span><span class="co"># Apply Bonferroni correction</span></span>
<span><span class="va">p_adjusted</span> <span class="op">&lt;-</span> <span class="fu">p.adjust</span><span class="op">(</span><span class="va">gene_pvalues</span><span class="op">$</span><span class="va">pvalue</span>, method <span class="op">=</span> <span class="st">"bonferroni"</span><span class="op">)</span></span>
<span><span class="va">significant_bonferroni</span><span class="op">&lt;-</span> <span class="va">p_adjusted</span> <span class="op">&lt;</span> <span class="va">alpha</span></span>
<span><span class="va">bonferroni_genes</span><span class="op">&lt;-</span><span class="fu">sum</span><span class="op">(</span><span class="va">significant_bonferroni</span><span class="op">)</span> <span class="co"># Number of significant hits after bonferroni correction</span></span>
<span><span class="va">bonferroni_genes</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 759</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>706 is the number of significant differentially expressed genes after Bonferroni correction for FWER</code></pre>
</div>
<p>Applying the Bonferroni correction in this example results in
<strong>706</strong> differentially expressed genes due to the stringent
threshold, demonstrating how FWER correction can be too conservative in
large-scale testing.</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>2357 is the number of significant differentially expressed genes that pass the FDR threshold</code></pre>
</div>
<p>The Benjamini-Hochberg method controls the FDR, allowing for a
greater number of significant differentially expressed genes
(<strong>2357</strong>) compared to the Bonferroni correction
(<strong>706</strong>). This approach provides a more balanced and
powerful method for identifying true positives in large-scale data.</p>
<div class="section level2">
<h2 id="advantages-of-controlling-the-fdr-over-fwer-in-the-context-of-large-scale-genomic-data">Advantages of controlling the FDR over FWER in the context of
large-scale genomic data<a class="anchor" aria-label="anchor" href="#advantages-of-controlling-the-fdr-over-fwer-in-the-context-of-large-scale-genomic-data"></a>
</h2>
<ul>
<li><p>Higher statistical power: FDR control allows for higher power
compared to FWER control. In large-scale genomic studies, this means
detecting more true positives while still controlling false discoveries.
Therefore, since FDR methods are less stringent than FWER methods like
Bonferroni correction, they balance between controlling false
discoveries and maximizing true positives, reducing false
negatives.</p></li>
<li><p>Consistency across studies: By setting a fixed FDR threshold
(e.g., 5%), results are more comparable across different studies and
meta-analyses, enhancing consistency in statistical inference.</p></li>
</ul>
<p>It is important to note that while FDR is preferred in many genomic
screens due to its ability to handle a large number of hypotheses, FWER
control remains appropriate in scenarios where minimizing any false
positives is critical. Each method has its place depending on the
research context and goals.</p>
</div>
<div class="section level2">
<h2 id="what-control-method-should-we-choose">What control method should we choose?<a class="anchor" aria-label="anchor" href="#what-control-method-should-we-choose"></a>
</h2>
<p>It is essential to understand the difference between controlling the
False Discovery Rate (FDR) and the Family-Wise Error Rate (FWER) as each
applies to different research scenarios.</p>
<p>Controlling the FDR is suitable in situations where it is often
acceptable to have some false positives, such as in large-scale
biological screenings (e.g., genomics, proteomics). The goal is to
identify as many true positives as possible, knowing that some false
positives can be filtered out in subsequent validation steps.</p>
<p>On the other hand, controlling FWER is suitable in high-precision
scenarios requiring strict control of false positives. In scenarios
where false positives can have significant consequences (e.g., clinical
trials, diagnostic tests), it is crucial to minimize even a single false
positive. Here, FWER control is more appropriate because it ensures a
lower risk of making any type I error.</p>
<p>Therefore, it is important to understand that in exploratory
research, where the aim is to generate hypotheses or discover potential
leads, FDR control is preferred due to its higher power. In confirmatory
research, where results need to be highly reliable, FWER control is more
suitable.</p>
<div class="section level3">
<h3 id="take-home-message">Take home message<a class="anchor" aria-label="anchor" href="#take-home-message"></a>
</h3>
<p>While FDR control is widely used in biological research due to its
balance between discovery and false positive control, FWER control is
crucial in settings where the cost of even a single false positive is
high. Each method serves its purpose depending on the specific goals and
acceptable risk levels of the research context. Understanding the
trade-offs between these approaches allows researchers to choose the
most appropriate method for their specific scenario.</p>
</div>
</div>
<div class="section level2">
<h2 id="further-reading">Further reading<a class="anchor" aria-label="anchor" href="#further-reading"></a>
</h2>
<ul>
<li><a href="%22http://varianceexplained.org/statistics/interpreting-pvalue-histogram/%22" class="external-link">How
to interpret a p-value histogram</a></li>
</ul>
<div id="accordionInstructor1" class="accordion instructor-note accordion-flush">
<div class="accordion-item">
<button class="accordion-button instructor-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseInstructor1" aria-expanded="false" aria-controls="collapseInstructor1">
  <h3 class="accordion-header" id="headingInstructor1">  <div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="edit-2"></i></div> Instructor Note </h3>
</button>
<div id="collapseInstructor1" class="accordion-collapse collapse" data-bs-parent="#accordionInstructor1" aria-labelledby="headingInstructor1">
<div class="accordion-body">
<p>If there’s too much time at hand, one could also make a quiz out of
the different histograms shown in the varianceexplained post, and
discuss what they could mean.</p>
</div>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</div></section><section id="aio-Pairwise-comparisons"><p>Content from <a href="Pairwise-comparisons.html">Pairwise comparisons</a></p>
<hr>
<p>Last updated on 2024-07-23 |

        <a href="https://example.com/FIXME-template-source/edit/main/episodes/Pairwise-comparisons.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are pairwise comparisons, and how do they relate to the broader
concept of multiple testing in statistical analysis?</li>
<li>How can we effectively conduct and interpret pairwise comparisons to
make valid statistical inferences while controlling for the family-wise
error rate?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand the Concept of Pairwise Comparisons</li>
<li>Learn how to conduct and interpret Pairwise Comparisons</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="pairwise-comparisons">Pairwise comparisons<a class="anchor" aria-label="anchor" href="#pairwise-comparisons"></a>
</h1>
<p>Pairwise comparisons are a fundamental concept in statistical
analysis, allowing us to compare the means or proportions of multiple
groups or conditions. In this episode, we will explore the concept of
pairwise comparisons, their importance in statistical testing, and how
they relate to the broader context of multiple testing.</p>
<div class="section level2">
<h2 id="understanding-pairwise-comparisons">Understanding Pairwise Comparisons<a class="anchor" aria-label="anchor" href="#understanding-pairwise-comparisons"></a>
</h2>
<p>In our example of air pollution, fine particulate matter and other
pollutants can induce systemic inflammation in the body, leading to an
increase in circulating inflammatory markers like C-reactive protein
(CRP). Suppose we decide to divide the population into four groups based
on different levels of exposure to air pollution. Lets assume these
groups are as follow:</p>
<ul>
<li>Low exposure: Individuals living in rural areas with minimal
industrial activity and low traffic density.</li>
<li>Moderate exposure: Individuals living in suburban areas with some
industrial activity and moderate traffic density.</li>
<li>High exposure: Individuals living in urban areas with significant
industrial activity and high traffic density.</li>
<li>Very high exposure: Individuals living near industrial zones, major
highways, or heavily polluted urban areas.</li>
</ul>
<p><strong>Table 2</strong> shows the first six rows of CRP values of
individuals in the four exposure groups.</p>
<table class="table">
<caption>Table 2: CRP values of individuals in the four exposure
groups</caption>
<thead><tr class="header">
<th align="right">CRP</th>
<th align="left">Exposure</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">1.379049</td>
<td align="left">Low_exposure</td>
</tr>
<tr class="even">
<td align="right">2.039645</td>
<td align="left">Low_exposure</td>
</tr>
<tr class="odd">
<td align="right">5.617417</td>
<td align="left">Low_exposure</td>
</tr>
<tr class="even">
<td align="right">2.641017</td>
<td align="left">Low_exposure</td>
</tr>
<tr class="odd">
<td align="right">2.758576</td>
<td align="left">Low_exposure</td>
</tr>
<tr class="even">
<td align="right">5.930130</td>
<td align="left">Low_exposure</td>
</tr>
</tbody>
</table>
<p>In this case the “Exposure” column is a categorical variable with
four groups of exposure levels (Low_exposure, Moderate_exposure,
High_exposure, and Very_high_exposure).</p>
<figure><img src="../fig/Pairwise-comparisons-rendered-unnamed-chunk-2-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Generally, from the above plot, as exposure level increases from low
to very high, we might expect to see a corresponding increase in the
median exposure level and higher median C-reactive protein (CRP)
values.</p>
<p>Our null hypothesis is that there is no difference in CRP amounts in
the different groups. Our significance levels is 0.05.</p>
<p>First we would do ANOVA, which would tell us whether there are any
differences between the groups.</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>             Df Sum Sq Mean Sq F value  Pr(&gt;F)
Exposure      3   60.8  20.268   5.754 0.00086 ***
Residuals   196  690.4   3.523
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
<p>The ANOVA output above, shows statistically significant differences
among the means of the four exposure groups. It does this by comparing
the variability within each group to the variability between the groups.
We therefore reject the null hypothesis of equal means.</p>
</div>
<div class="section level2">
<h2 id="why-do-we-need-multiple-comparisons">Why do we need multiple comparisons?<a class="anchor" aria-label="anchor" href="#why-do-we-need-multiple-comparisons"></a>
</h2>
<p>ANOVA alone does not tell us which specific groups differ
significantly from each other. To determine this, we need to conduct
further analysis. Multiple comparison procedures (also known as post-hoc
tests) are used to compare pairs of groups in order to identify where
the differences lie. These tests provide more detailed information about
which specific group means are different from each other.</p>
<div class="section level3">
<h3 id="methods-for-performing-pairwise-comparisons">Methods for Performing Pairwise Comparisons<a class="anchor" aria-label="anchor" href="#methods-for-performing-pairwise-comparisons"></a>
</h3>
<p>Parametric methods, such as ANOVA post-hoc tests (e.g., Tukey’s HSD)
are used when assumptions of normality and homogeneity of variance are
met. Non-parametric methods, such as pairwise Wilcoxon tests, are
suitable when data do not meet the assumptions of parametric tests. In
R, performing pairwise comparisons is straightforward using built-in
functions and packages.</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = CRP ~ Exposure, data = data)

$Exposure
                                           diff        lwr       upr     p adj
Moderate_exposure-Low_exposure        0.7240094 -0.2486521 1.6966710 0.2193881
High_exposure-Low_exposure            0.4233920 -0.5492695 1.3960536 0.6727404
Very_high_exposure-Low_exposure       1.5088066  0.5361450 2.4814681 0.0004795
High_exposure-Moderate_exposure      -0.3006174 -1.2732790 0.6720442 0.8539510
Very_high_exposure-Moderate_exposure  0.7847971 -0.1878644 1.7574587 0.1597266
Very_high_exposure-High_exposure      1.0854145  0.1127530 2.0580761 0.0219956</code></pre>
</div>
<p>We perform a Tukey post-hoc test using <strong>TukeyHSD()</strong> to
determine which specific exposure group means differ significantly from
each other. The results provide pairwise comparisons of group means
along with adjusted p-values to account for multiple comparisons.</p>
</div>
<div class="section level3">
<h3 id="interpreting-pairwise-comparison-results">Interpreting Pairwise Comparison Results<a class="anchor" aria-label="anchor" href="#interpreting-pairwise-comparison-results"></a>
</h3>
<p>Understanding significance levels and p-values obtained from pairwise
comparisons is essential for interpreting results accurately. The
p-values obtained from pairwise comparisons using the Tukey test are
adjusted to control the familywise error rate (FWER). Adjusted p-values
reflect the probability of observing a given result (or more extreme)
under the assumption that all null hypotheses are true, while accounting
for the number of comparisons made. Here, we interpret pairwise
comparison results with adjusted p-values from the Tukey test by
assessing whether the adjusted p-value is less than the chosen
significance level (α) (0.05). If the adjusted p-value is below the
significance level, we conclude that the observed difference is
statistically significant after correcting for multiple comparisons.</p>
<p>In some research scenarios, the comparisons between groups are
designed to answer specific, pre-defined research questions that are not
directly addressed by traditional ANOVA and subsequent p-value
adjustments. In this case, ANOVA and p-value adjustment don’t make any
sense.</p>
<p>For example, imagine we are conducting a study to investigate the
effect of different types of exercise (A, B, C, D) on three different
health outcomes (X, Y, Z). Our primary interest here is to compare each
type of exercise directly with respect to each health outcome, rather
than comparing all exercises together using ANOVA.</p>
<p>Probable research questions would be:</p>
<ul>
<li>Does exercise type A have a different effect than exercise type B on
health outcome X?</li>
<li>Is there a difference in the effect of exercise type A compared to
exercise type C on health outcome Y?</li>
<li>How does exercise type D compare to exercise type B in terms of
health outcome Z?</li>
</ul>
<p>In this case, instead of conducting a single ANOVA followed by
post-hoc tests to compare all exercises simultaneously, each comparison
is treated as a separate research question. The focus is on comparing
specific pairs of exercise types for each health outcome.</p>
<p>This means that we would analyze each comparison (e.g., A vs B for X,
A vs C for Y, D vs B for Z) independently using appropriate statistical
tests suited for comparing two groups (e.g., t-tests, Wilcoxon rank-sum
tests, etc.). Since these comparisons are independent and are addressing
distinct research questions, there is no need for ANOVA or adjustment of
p-values across comparisons.</p>
<p>We would then interpret the results of each pairwise comparison based
on their individual statistical significance and effect sizes relevant
to the specific research question posed. This approach allows for a
focused investigation into the differences between specific pairs of
exercise types and their effects on different health outcomes. This
tailored approach ensures that the statistical analysis aligns closely
with the specific objectives of the study, providing meaningful insights
into the relationships between variables of interest.</p>
</div>
</div>
<div class="section level2">
<h2 id="what-we-learn">What we learn<a class="anchor" aria-label="anchor" href="#what-we-learn"></a>
</h2>
<p>Pairwise comparisons involve comparing the means or proportions of
every possible pair of groups or conditions in a dataset. These
comparisons are used to identify specific differences between groups or
conditions that may not be apparent from overall statistical tests (like
the above case ANOVA). These comparisons play a crucial role in
statistical testing, particularly in the context of multiple
comparisons. They allow us to examine specific comparisons of interest
while controlling for the overall familywise error rate. The comparisons
are commonly used in various fields, including medicine, biology, social
sciences, and economics, to compare treatment groups, experimental
conditions, or categorical variables.</p>
</div>
<div class="section level2">
<h2 id="data-snooping-figure_6-data-snooping">Data snooping <img src="../fig/Data%20snooping.png" alt="Figure_6: Data snooping" class="figure"><a class="anchor" aria-label="anchor" href="#data-snooping-figure_6-data-snooping"></a>
</h2>
<p>In the context of the above analysis, data snooping refers to the
practice of exploring the data extensively, testing multiple hypotheses,
and making comparisons until finding a statistically significant result
or an interesting pattern. This can lead to overfitting the data or
finding false positives due to chance alone.</p>
<p>In our example, data snooping could involve testing multiple
comparisons between exposure groups until finding one that appears to be
statistically significant or interesting. For example, one might compare
only the “Very_high_exposure” group with the other groups, ignoring
other potential comparisons. This selective comparison increases the
likelihood of finding a significant result by chance alone.</p>
<p>Additionally, in some cases, we might decide not to perform all
tests. We could just visually inspect our plot, and only perform the
test that seems most promising when comparing the bars by eye. For
example, after observing the initial boxplot, we might want to conduct
numerous additional analyses or subgroup comparisons based on observed
patterns, without pre-specifying these comparisons. This approach can
lead to inflated Type I error rates (false positives) if corrections for
multiple testing are not applied.</p>
<p>Data snooping can influence decision-making by focusing only on
results that appear favorable or intriguing in the data. For instance,
if a particular exposure group shows a larger mean than others, a
decision might be made to focus solely on interventions or policies
targeting that group without considering the broader context or
potential confounding factors.</p>
</div>
<div class="section level2">
<h2 id="additional-resources">Additional resources<a class="anchor" aria-label="anchor" href="#additional-resources"></a>
</h2>
<p>For further reading and hands-on practice with pairwise comparisons
in R, refer to <a href="%22https://www.rdocumentation.org/%22" class="external-link">R
documentation on pairwise comparisons</a>, <a href="%22https://www.coursera.org/specializations/statistics%22" class="external-link">Applied
Statistics with R</a> and <a href="%22http://users.stat.umn.edu/~gary/book/fcdae.pdf%22" class="external-link">book by
Oehlert 2010</a> resources.</p>
<div id="accordionInstructor1" class="accordion instructor-note accordion-flush">
<div class="accordion-item">
<button class="accordion-button instructor-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseInstructor1" aria-expanded="false" aria-controls="collapseInstructor1">
  <h3 class="accordion-header" id="headingInstructor1">  <div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="edit-2"></i></div> Instructor Note </h3>
</button>
<div id="collapseInstructor1" class="accordion-collapse collapse" aria-labelledby="headingInstructor1" data-bs-parent="#accordionInstructor1">
<div class="accordion-body">
<p>Inline instructor notes can help inform instructors of timing
challenges associated with the lessons. They appear in the “Instructor
View”</p>
</div>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</div></section><section id="aio-Summary"><p>Content from <a href="Summary.html">Summary</a></p>
<hr>
<p>Last updated on 2024-07-23 |

        <a href="https://example.com/FIXME-template-source/edit/main/episodes/Summary.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="section level1">
<h1 id="error-rates">Error rates<a class="anchor" aria-label="anchor" href="#error-rates"></a>
</h1>
<p>We have seen that when dealing with multiple testing, it is crucial
to understand the different types of error rates that can be controlled.
<strong>Comparison-Wise Error Rate (CWER)</strong>, which is the
probability of making a Type I error (false positive) in any single
hypothesis test, does not account for the multiplicity of tests and can
lead to an increased number of false positives when many tests are
performed. <strong>Family-Wise Error Rate (FWER)</strong>, the
probability of making at least one Type I error among all the hypothesis
tests, is a stringent error rate control method, suitable for situations
where even one false positive is highly problematic. Methods like the
Bonferroni correction and Holm’s are often used to control FWER.
<strong>False Discovery Rate (FDR)</strong>, the expected proportion of
Type I errors among the rejected hypotheses, is less conservative than
FWER and is useful in large-scale testing scenarios, like genomic
studies, where some false positives are tolerable in exchange for higher
power to detect true positives. The Benjamini-Hochberg procedure is a
common method to control the FDR.</p>
</div>
<div class="section level1">
<h1 id="choosing-the-appropriate-error-rate">Choosing the appropriate error rate<a class="anchor" aria-label="anchor" href="#choosing-the-appropriate-error-rate"></a>
</h1>
<p>The choice of error rate to control depends on the research question
and the context of the study. For example, we could choose to control
CWER when testing the efficacy of a single new drug against a placebo in
a clinical trial, or when performing a single regression analysis to
test the relationship between two variables.</p>
<p>One would consider controlling FWER in clinical trials involving
multiple endpoints, where any false positive could lead to incorrect
conclusions about the drug’s safety or efficacy. in addition,
psychological experiments where multiple outcomes are tested and any
false positive could mislead theoretical interpretations would consider
controlling FWER.</p>
<p>The choise for controlling FDR is often considered in genome-wide
association studies (GWAS) analyzing thousands of genetic variants
simultaneously to identify potential associations with a disease, or in
RNA-Seq experiments aiming to identify differentially expressed genes
among tens of thousands of genes.</p>
</div>
<div class="section level1">
<h1 id="a-cook-book-to-navigate-multiple-testing-effectively">A Cook book to navigate multiple testing effectively<a class="anchor" aria-label="anchor" href="#a-cook-book-to-navigate-multiple-testing-effectively"></a>
</h1>
<p>To navigate multiple testing effectively, we should:</p>
<ol style="list-style-type: decimal">
<li><p>start by <strong>clearly defining our research question</strong>,
stating the specific hypothesis or set of hypotheses we aim to test, and
considering the implications of potential false positives and false
negatives.</p></li>
<li><p>Next, we <strong>decide on the appropriate error rate to
control</strong>, based on our research context and tolerance for Type I
errors. this means assessing how critical it is to avoid any false
positives (favoring FWER) versus allowing some false positives to
increase detection power (favoring FDR).</p></li>
<li><p>Finally, we <strong>select a suitable statistical method that
controls our chosen error rate</strong> and aligns with our data type
and experimental design, ensuring it is well-suited to the specifics of
our study.</p></li>
</ol>
<div id="accordionInstructor1" class="accordion instructor-note accordion-flush">
<div class="accordion-item">
<button class="accordion-button instructor-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseInstructor1" aria-expanded="false" aria-controls="collapseInstructor1">
  <h3 class="accordion-header" id="headingInstructor1">  <div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="edit-2"></i></div> Instructor Note </h3>
</button>
<div id="collapseInstructor1" class="accordion-collapse collapse" data-bs-parent="#accordionInstructor1" aria-labelledby="headingInstructor1">
<div class="accordion-body">
<p>Inline instructor notes can help inform instructors of timing
challenges associated with the lessons. They appear in the “Instructor
View”</p>
</div>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div></section><section id="aio-References"><p>Content from <a href="References.html">References</a></p>
<hr>
<p>Last updated on 2024-07-23 |

        <a href="https://example.com/FIXME-template-source/edit/main/episodes/References.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="section level1">
<h1 id="books">Books<a class="anchor" aria-label="anchor" href="#books"></a>
</h1>
<ul>
<li>Westfall, P. H., &amp; Young, S. S. (1993). Resampling-based
multiple testing: Examples and methods for p-value adjustment (Vol.
279). John Wiley &amp; Sons.</li>
<li>Hochberg, Y. (1987). Multiple comparison procedures. Wiley Series in
Probability and Statistics.</li>
<li>Dudoit, S., Van Der Laan, M. J., &amp; van der Laan, M. J. (2008).
Multiple testing procedures with applications to genomics
(pp. XXXIII-588). New York: Springer.</li>
</ul>
</div>
<div class="section level1">
<h1 id="articles">Articles<a class="anchor" aria-label="anchor" href="#articles"></a>
</h1>
<ul>
<li>Benjamini, Y., &amp; Hochberg, Y. (1995). Controlling the false
discovery rate: a practical and powerful approach to multiple testing.
Journal of the Royal statistical society: series B (Methodological),
57(1), 289-300..</li>
<li>Holm, S. (1979). A simple sequentially rejective multiple test
procedure. Scandinavian journal of statistics, 65-70.</li>
<li>Shaffer, J. P. (1995). Multiple hypothesis testing. Annual review of
psychology, 46(1), 561-584.</li>
<li>Storey, J. D., &amp; Tibshirani, R. (2003). Statistical significance
for genomewide studies. Proceedings of the National Academy of Sciences,
100(16), 9440-9445.</li>
<li>Pollard, K. S., Dudoit, S., &amp; van der Laan, M. J. (2005).
Multiple testing procedures: the multtest package and applications to
genomics. In Bioinformatics and computational biology solutions using R
and bioconductor (pp. 249-271). New York, NY: Springer New York.</li>
</ul>
</div>
<div class="section level1">
<h1 id="online-resources">Online Resources<a class="anchor" aria-label="anchor" href="#online-resources"></a>
</h1>
<ul>
<li><p><a href="%22https://biodatascience.github.io/compbio/test/multtest.html%22" class="external-link">Introduction
to multiple testing</a></p></li>
<li><p><a href="%22https://journal.r-project.org/archive/2014/RJ-2014-027/RJ-2014-027.pdf%22" class="external-link">sgof:
An R Package for Multiple Testing Problems</a></p></li>
</ul>
<div id="accordionInstructor1" class="accordion instructor-note accordion-flush">
<div class="accordion-item">
<button class="accordion-button instructor-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseInstructor1" aria-expanded="false" aria-controls="collapseInstructor1">
  <h3 class="accordion-header" id="headingInstructor1">  <div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="edit-2"></i></div> Instructor Note </h3>
</button>
<div id="collapseInstructor1" class="accordion-collapse collapse" data-bs-parent="#accordionInstructor1" aria-labelledby="headingInstructor1">
<div class="accordion-body">
<p>Inline instructor notes can help inform instructors of timing
challenges associated with the lessons. They appear in the “Instructor
View”</p>
</div>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://example.com/FIXME-template-source/edit/main/README.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://example.com/FIXME-template-source/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://example.com/FIXME-template-source/" class="external-link">Source</a></p>
				<p><a href="https://example.com/FIXME-template-source/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:sarah.kaspar@embl.de">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.5" class="external-link">sandpaper (0.16.5)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.6" class="external-link">pegboard (0.7.6)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.3" class="external-link">varnish (1.0.3)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://FIXME-template-source.github.io/NA/instructor/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, statistics",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://FIXME-template-source.github.io/NA/instructor/aio.html",
  "identifier": "https://FIXME-template-source.github.io/NA/instructor/aio.html",
  "dateCreated": "2024-04-18",
  "dateModified": "2024-07-23",
  "datePublished": "2024-07-23"
}

  </script><script>
		feather.replace();
	</script>
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

