<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Multiple Testing: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css">
<script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="site.webmanifest">
<link rel="mask-icon" href="safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="black">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
<li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul>
</li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/aio.html';">Instructor View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Multiple Testing
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Multiple Testing
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Multiple Testing
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text">
<li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul>
</li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/aio.html">Instructor View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->

            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="What-is-multiple-testing.html">1. What is multiple testing</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="Types-of-errors.html">2. Types of errors and error rates</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="Family-wise-error-rate.html">3. Family-wise error rate</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="False-discovery-rate.html">4. False discovery rate</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="Pairwise-comparisons.html">5. Pairwise comparisons</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="Summary.html">6. Summary</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="References.html">7. References and Further Reading</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>
                      <li><a href="reference.html">Reference</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources">
<a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">

            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-What-is-multiple-testing"><p>Content from <a href="What-is-multiple-testing.html">What is multiple testing</a></p>
<hr>
<p>Last updated on 2024-09-17 |

        <a href="https://example.com/FIXME-template-source/edit/main/episodes/What-is-multiple-testing.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is multiple testing?</li>
<li>How are false-positives related to the significance level?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Define multiple testing</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="what-is-multiple-testing">What is multiple testing<a class="anchor" aria-label="anchor" href="#what-is-multiple-testing"></a>
</h1>
<p>Let’s approach this question through an example and a recap on
hyptothesis testing: Suppose the prevalence of a disease in the general
population is 4%. In this population, lives a group of individuals who
have all been exposed to air pollution. Concerned about their health, we
decide to embark on a quest to uncover whether being exposed to air
pollution influenced the risk of contracting this disease.</p>
<figure><img src="fig/01-Disease-prevalence.png" alt="Disease prevalence in a population: We compare the known population average of 4% to a test group in which 9/100 individuals have the disease." class="figure mx-auto d-block"><div class="figcaption">Disease prevalence in a population: We compare
the known population average of 4% to a test group in which 9/100
individuals have the disease.</div>
</figure><div class="section level2">
<h2 id="setting-the-null-and-alternative-hypothesis">Setting the null and alternative hypothesis<a class="anchor" aria-label="anchor" href="#setting-the-null-and-alternative-hypothesis"></a>
</h2>
<p>We would like to conduct a hypothesis test to find out whether the
prevalence in the test group differs from the known 4%. For this, we
define the null and the alternative hypothesis.</p>
<div class="section level3">
<h3 id="null-hypothesis-h_0">Null Hypothesis (<span class="math inline">\(H_0\)</span>)<a class="anchor" aria-label="anchor" href="#null-hypothesis-h_0"></a>
</h3>
<p>The prevalence of the disease within test group exposed to air
pollution is the same as the known prevalence in the general population
(4%). This means that the proportion of individuals exposed to air
pollution in the test group who have the disease is also 4%.</p>
</div>
<div class="section level3">
<h3 id="alternative-hypothesis-h_1">Alternative Hypothesis (<span class="math inline">\(H_1\)</span>)<a class="anchor" aria-label="anchor" href="#alternative-hypothesis-h_1"></a>
</h3>
<p>The prevalence of the disease within the test group exposed to air
pollution is different from the known prevalence in the general
population. This means that the proportion of individuals exposed to air
pollution in the test group who have the disease is either higher or
lower than 4%.</p>
</div>
</div>
<div class="section level2">
<h2 id="data-collection-and-testing">Data collection and testing<a class="anchor" aria-label="anchor" href="#data-collection-and-testing"></a>
</h2>
<p>We assemble a group of 100 individuals who have been exposed to air
pollution (test group) from the population and each individual is
carefully examined, checking for any signs of the disease. Out of the
100 individuals, we discover that 9 of them were indeed suffering from
the disease, so the <em>observed</em> proportion is 9%. This is
different from 4%, but we are not satisfied with just this knowledge,
since the observed difference in proportions could be due to chance. We
want to know if this prevalence of the disease within the group exposed
to air pollution was significantly different from the population’s
average, meaning that it’s very unlikely to observe this difference just
by chance. So, we decide to perform binomial test (please refer back to
<a href="%22https://sarahkaspar.github.io/biostatistics-course/03-binomial.html%22" class="external-link">binomial
test</a>). With this test, we compare the observed prevalence within our
group that has been exposed to air pollution to the known prevalence in
the entire population.</p>
<p>We set our significance level beforehand, typically at <span class="math inline">\(\alpha=0.05\)</span>, to determine whether the
results are statistically significant.</p>
<p>Here, we run the test in R:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#For known parameters (n=100, p=0.04), we calculates the the chances of getting the 9 individuals that indeed suffered from the disease. </span></span>
<span><span class="va">n</span> <span class="op">=</span> <span class="fl">100</span> <span class="co"># number of test persons</span></span>
<span><span class="va">p</span> <span class="op">=</span> <span class="fl">0.04</span> <span class="co"># Known prevalence of the disease in the general population</span></span>
<span><span class="fu">binom.test</span><span class="op">(</span>x<span class="op">=</span><span class="fl">9</span>, n<span class="op">=</span><span class="va">n</span>, p<span class="op">=</span><span class="va">p</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
	Exact binomial test

data:  9 and n
number of successes = 9, number of trials = 100, p-value = 0.01899
alternative hypothesis: true probability of success is not equal to 0.04
95 percent confidence interval:
 0.0419836 0.1639823
sample estimates:
probability of success
                  0.09 </code></pre>
</div>
<p>The obtained p-value is the probability of seeing an outcome as
extreme as the observed one, assuming that the null hypothesis is true.
If this probability is sufficiently low (below our chosen significance
level), we reject the null hypothesis in favor of the alternative
hypothesis.</p>
<p>The binomial test result (~0.02) reveals that the prevalence of the
disease among the individuals exposed to air pollution is indeed
significantly different from that of the population.</p>
</div>
<div class="section level2">
<h2 id="what-if-we-did-many-similar-experiments">What if we did many similar experiments?<a class="anchor" aria-label="anchor" href="#what-if-we-did-many-similar-experiments"></a>
</h2>
<p>Conducting a single study might not provide conclusive evidence due
to various factors such as sample variability, random chance, and other
unknown influences.</p>
<p>We decide to investigate the potential impact of air pollution on
disease prevalence in 200 various locations. We want to assess whether
there is a significant difference in disease rates between groups
exposed to air pollution and the average for the whole population.</p>
<p>Let’s now assume that in our scenario, there is <em>no</em>
association between air pollution and the disease in <em>any</em> of the
tested regions.</p>
<div id="vote" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="vote" class="callout-inner">
<h3 class="callout-title">Vote</h3>
<div class="callout-content">
<p>What kind of results would you expect from running a series of tests
in 200 regions?</p>
<ul>
<li>no significant test</li>
<li>all tests significant</li>
<li>some tests significant</li>
</ul>
</div>
</div>
</div>
<p>To find out, we can simulate the scenario where we conduct 200 tests,
each with a 5% chance (<span class="math inline">\(\alpha =
0.05\)</span>) of producing a significant result (i.e., a p-value less
than 0.05) even when the null hypothesis is true.</p>
<p>Our null hypothesis in each location is that there is no real
difference in disease rates between the groups exposed to air pollution
and the average for the whole population.</p>
<figure><img src="fig/Scenario_200_nulls.png" alt="A Scenario where 100 individuals get tested for a disease. The disease prevalence is 0.04. The experiment is repeated 200 times" class="figure mx-auto d-block"><div class="figcaption">A Scenario where 100 individuals get tested for
a disease. The disease prevalence is 0.04. The experiment is repeated
200 times</div>
</figure><p>To do this, we write a script in R, which simulates study results
when the prevalence in the test group is 4% (null hypothesis is true).
We run these experiments to see what would happen if we kept doing tests
even when there was not actually any difference.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="va">tidyverse</span><span class="op">)</span></span>
<span><span class="fu">set.seed</span><span class="op">(</span><span class="fl">33</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># define lower and upper bounds: beyond these bounds, results are significant</span></span>
<span><span class="va">lower</span> <span class="op">&lt;-</span> <span class="fu">qbinom</span><span class="op">(</span><span class="fl">0.025</span>, size<span class="op">=</span><span class="fl">100</span>, prob<span class="op">=</span><span class="fl">0.04</span><span class="op">)</span></span>
<span><span class="va">upper</span> <span class="op">&lt;-</span> <span class="fu">qbinom</span><span class="op">(</span><span class="fl">0.975</span>, size<span class="op">=</span><span class="fl">100</span>, prob<span class="op">=</span><span class="fl">0.04</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Generate simulated data</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu">data.frame</span><span class="op">(</span></span>
<span>  count <span class="op">=</span> <span class="fu">rbinom</span><span class="op">(</span>n<span class="op">=</span><span class="fl">200</span>, size<span class="op">=</span><span class="fl">100</span>, prob<span class="op">=</span><span class="fl">0.04</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># add significance</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">data</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>significant <span class="op">=</span> <span class="fu">ifelse</span><span class="op">(</span></span>
<span>    <span class="op">(</span><span class="va">count</span> <span class="op">&lt;</span> <span class="va">lower</span> <span class="op">|</span> <span class="va">count</span> <span class="op">&gt;</span> <span class="va">upper</span><span class="op">)</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">data</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">count</span>, fill<span class="op">=</span><span class="fu">factor</span><span class="op">(</span><span class="va">significant</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>binwidth<span class="op">=</span><span class="fl">1</span>,color<span class="op">=</span><span class="st">"black"</span><span class="op">)</span><span class="op">+</span></span>
<span>   <span class="fu">scale_fill_discrete</span><span class="op">(</span>type<span class="op">=</span><span class="fu">c</span><span class="op">(</span><span class="st">"darkslateblue"</span>,<span class="st">"lightblue"</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">guides</span><span class="op">(</span>fill <span class="op">=</span> <span class="st">"none"</span><span class="op">)</span></span></code></pre>
</div>
<p><img src="fig/What-is-multiple-testing-rendered-Simulating-200-test-groups-1.png" style="display: block; margin: auto;" class="figure">
What does the above code do:</p>
<ul>
<li>Determine lower and upper bounds for significance: A two-tailed
binomial test calls counts significant that are either below the 2.5
percentile, or above the 97.5 percentile of the binomial distribution.
We determine these bounds using <code>qbinom</code>. This gives us the
bounds that contain 95% of the data under the null hypothesis.</li>
<li>Determine significance: We then classify a group as
<code>significant</code> if the observed number of patients falls below
the 2.5th percentile or above the 97.5th percentile of the binomial
distribution.</li>
<li>Plotting: We plot a histogram of the counts, and use the argument
<code>fill</code> to color by the variable <code>significant</code>. We
use <code>scale_fill_discrete</code> to manually determine the colors
for significant and non-significant.</li>
</ul>
<p>In the resulting histogram, we find that even in a world where there
was no true difference in disease prevalence, about 5% of our simulated
experiments yielded statistically significant results purely by chance
(the light blue bars).</p>
<p>It is important to note that the significance level (α) that we
choose for each individual test directly impacts the rate of false
positives. This is the <strong>Comparison-Wise Error Rate
(CWER)</strong>, the probability of making a Type I error (false
positive) in a single hypothesis test. In our example, we have set
α=0.05 for each individual test, which means we are willing to accept a
5% chance of making a false positive error for each test, and this means
that for 100 tests, we expect about 5 false positives.</p>
<p>By running this simulation multiple times, we can observe how often
we get false positive results when there should be none. This helps us
understand the likelihood of obtaining a significant result purely by
chance, even if there is no true effect or difference.</p>
<div id="discussion2" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<ul>
<li>If we set α=0.01 for each individual test, what is the number of
false positives we should expect for 100 tests?</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>We expect 1 false positive on average.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level2">
<h2 id="summary">Summary<a class="anchor" aria-label="anchor" href="#summary"></a>
</h2>
<p>Multiple testing refers to the practice of conducting numerous
hypothesis tests simultaneously or repeatedly on the same data set. It
is typically motivated by the desire to explore different aspects of the
data or to investigate multiple hypotheses. Researchers employ multiple
tests to examine various relationships, comparisons, or associations
within their data set, such as comparing means, proportions,
correlations, or other statistical analyses that involve hypothesis
testing.</p>
<p>When we increase the number of conducted tests, this also increases
the number of false positives to be expected among the tests where the
null hypothesis is true. The <em>fraction</em> of expected false
positives among the cases with true null is equal to the significance
level <span class="math inline">\(\alpha\)</span> that we apply for each
individual test.</p>

<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</div></section><section id="aio-Types-of-errors"><p>Content from <a href="Types-of-errors.html">Types of errors and error rates</a></p>
<hr>
<p>Last updated on 2024-09-17 |

        <a href="https://example.com/FIXME-template-source/edit/main/episodes/Types-of-errors.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are false positives and false negatives and how do they
manifest in a confusion matrix?</li>
<li>What are some of real examples where false positives and false
negatives have different implications and consequences?</li>
<li>What are type I error rates?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand the concept of false positives and false negatives and
how they are represented in a confusion matrix</li>
<li>Analyse and discuss scenarios where false positives and false
negatives pose distinct challenges and implications</li>
<li>Highlight situations where minimizing each type of error is
crucial</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="types-of-errors">Types of errors<a class="anchor" aria-label="anchor" href="#types-of-errors"></a>
</h1>
<p>In hypothesis testing, the evaluation of statistical significance
involves making decisions based on sampled data. However, these
decisions are not without errors. In this tutorial, we will explore the
concept of errors in hypothesis testing (Type I and Type II errors), and
their implications.</p>
<div class="section level2">
<h2 id="type-i-error">Type I Error<a class="anchor" aria-label="anchor" href="#type-i-error"></a>
</h2>
<p>A type I error occurs when we reject a null hypothesis that is
actually true. For example, in our previous example where we conducted
many similar experiments, we experienced type I error by concluding that
exposure to air pollution has an effect on disease prevalence (rejecting
the null hypothesis) when it actually has no effect. Type I errors
represent false positives and can lead to incorrect conclusions,
potentially resulting in wasted resources or misguided decisions.</p>
</div>
<div class="section level2">
<h2 id="type-ii-error">Type II Error<a class="anchor" aria-label="anchor" href="#type-ii-error"></a>
</h2>
<p>A type II error occurs when we fail to reject a null hypothesis that
is actually false. For example, when we fail to conclude that exposure
to air pollution has an effect on disease prevalence (failing to reject
the null hypothesis) when it actually has a positive effect. Type II
errors represent false negatives and can result in missed opportunities
or overlooking significant effects.</p>
<p>Type II errors often occur when in settings with low
<strong>statistical power</strong> is chosen. Statistical power refers
to the probability of correctly rejecting the null hypothesis when it is
indeed false. In simpler terms, it measures the likelihood of detecting
a true effect or difference if it exists. Low power can be due to
properties of the sample, such as insufficient number of data points, or
high variance. But there are also tests that are better at detecting an
effect than others in certain settings.</p>
</div>
<div class="section level2">
<h2 id="confusion-matrix">Confusion Matrix<a class="anchor" aria-label="anchor" href="#confusion-matrix"></a>
</h2>
<p>In the context of hypothesis testing, we can conceptualize Type I and
Type II errors using a confusion matrix. The confusion matrix represents
the outcomes of hypothesis testing as True Positives (correctly
rejecting H0), False Positives (incorrectly rejecting H0), True
Negatives (correctly failing to reject H0), and False Negatives
(incorrectly failing to reject H0).</p>
<figure><img src="fig/Figure%201%20Errors%20in%20hypothesis%20testing.png" alt="Confusion matrix" class="figure mx-auto d-block"><div class="figcaption">Confusion matrix</div>
</figure>
</div>
</div>
<div class="section level1">
<h1 id="implications-of-type-i-and-type-ii-errors">Implications of type I and type II errors<a class="anchor" aria-label="anchor" href="#implications-of-type-i-and-type-ii-errors"></a>
</h1>
<p>In hypothesis testing, the occurrence of Type I and Type II errors
can have different implications depending on the context of the problem
being addressed. It is crucial to understand which errors are
problematic in which situation to be able to make informed decisions and
draw accurate conclusions from statistical analyses.</p>
<p>Type I errors are particularly problematic in situations where the
cost or consequences of incorrectly rejecting a true null hypothesis are
high. Again, if we refer back to our example, if we incorrectly conclude
that there is a significant difference in disease rates between the test
groups exposed to air pollution and the average for the whole population
when, in fact, there is no such difference, it could lead to misguided
policies or interventions targeting air pollution reduction. For
instance, authorities might implement costly environmental regulations
or public health measures based on erroneous conclusions. In this case,
the <strong>consequences</strong> include misallocation of resources,
leading to unnecessary financial burdens or societal disruptions.
Moreover, public trust in scientific findings and policy decisions may
be eroded if false positives lead to ineffective or burdensome
interventions.</p>
<p>Type II errors are problematic when failing to detect a significant
effect has substantial consequences. If we fail to detect a significant
difference in disease rates between the test group and the population
average when there actually is a difference due to air pollution
exposure, it could result in overlooking a serious public health
concern. In this case, individuals living in polluted areas may continue
to suffer adverse health effects without receiving appropriate attention
or interventions. The <strong>consequences</strong> include increased
morbidity and mortality rates among populations exposed to high levels
of air pollution. Additionally, delayed or inadequate response to
environmental health risks may exacerbate inequalities in health
outcomes.</p>
<div class="section level2">
<h2 id="an-example-of-cancer-screening">An example of cancer screening<a class="anchor" aria-label="anchor" href="#an-example-of-cancer-screening"></a>
</h2>
<p>Cancer screening exemplifies a medical testing paradox, where the
interpretation of test results can be influenced by factors such as
disease prevalence, test sensitivity, and specificity.</p>
<p>Let us say that in a sample of 1000 women, 1% (10) have cancer, while
the remaining 99% (990) do not have cancer. This gives us the prevalence
of a disease. However, after testing, the test results show that out of
the 10 women with cancer, 9 receive a <strong>true positive</strong>
result (correctly identified as positive), and 1 receives a
<strong>false negative</strong> result (incorrectly identified as
negative). False negatives can delay the diagnosis and treatment of
cancer, allowing the disease to progress unchecked and potentially
reducing the effectiveness of treatment options. This can result in
poorer outcomes and decreased survival rates for patients. In addition,
out of the 990 women without cancer, 89 receive <strong>false
positive</strong> results (incorrectly identified as positive), and 901
receive <strong>true negative</strong> results (correctly identified as
negative). False positive can lead to unnecessary follow-up tests,
procedures, and treatments for individuals who do not have cancer. It
can cause anxiety, physical discomfort, and financial burden for
patients, as well as strain on healthcare resources.</p>
<figure><img src="fig/cancer-paradox.png" alt="A tree diagram describing the outcomes of a breast cancer test" class="figure mx-auto d-block"><div class="figcaption">A tree diagram describing the outcomes of a
breast cancer test</div>
</figure><p>We could interpret that:</p>
<ul>
<li><p>The <strong>probability</strong> that a woman who receives a
positive result actually has cancer is ≈ 1/10 ( 9/ (9 + 89)). This is
the positive predictive value (PPV) of the test.</p></li>
<li><p>The <strong>sensitivity</strong> of the test, which measures its
ability to detect the presence of disease is 90% (9/10 * 100). This
means that the false negative rate of the test is 10%.</p></li>
<li><p>The <strong>specificity</strong> of the test, which measures its
ability to correctly identify individuals without the disease is ≈ 91%
(901/990 * 100). Here, the false positive rate of the test is
9%.</p></li>
</ul>
<p>While the test may have high accuracy in terms of sensitivity and
specificity, the positive predictive value is relatively low due to the
low prevalence of the disease in the population. This means that a
positive result from the test does not strongly predict the presence of
the disease in an individual. Similarly, false positives and false
negatives can affect the negative predictive value of the test, which
measures its ability to correctly identify individuals who do not have
cancer. False negatives decrease the negative predictive value, while
false positives increase it, potentially leading to misinterpretation of
test results.</p>
<p>This example underscores the complexity of interpreting medical test
results and emphasizes the need to consider factors such as disease
prevalence, test sensitivity, and specificity in clinical
decision-making. Increasing sensitivity may reduce false negatives but
can also increase false positives, and vice versa. Thus, optimizing the
trade-off between sensitivity and specificity is crucial to minimize
false positives and false negatives while maximizing the accuracy of the
screening test.</p>
<div class="section level3">
<h3 id="what-do-we-learn">What do we learn?<a class="anchor" aria-label="anchor" href="#what-do-we-learn"></a>
</h3>
<p>In many real-world scenarios, there is a trade-off between Type I and
Type II errors, and the appropriate balance depends on the specific
goals and constraints of the problem. Reseachers may prioritize one over
the other based on the severity of the consequences. For example, in
cancer screenings, minimizing false negatives (Type II errors) is
typically prioritized to avoid missing potential cases of cancer, even
if it leads to an increase in false positives (Type I errors).</p>
<p>Effective evaluation of Type I and Type II errors necessitates a
comprehensive consideration of the associated costs, risks, and ethical
implications. This holistic approach enhances the validity and
reliability of research findings by ensuring that decisions regarding
hypothesis acceptance or rejection are informed not only by statistical
significance but also by the potential real-world consequences of both
false positives and false negatives. By carefully weighing these
factors, researchers can make informed decisions that minimize the
likelihood of erroneous conclusions while maximizing the practical
relevance and ethical soundness of their findings.</p>
</div>
</div>
<div class="section level2">
<h2 id="error-rates">Error rates<a class="anchor" aria-label="anchor" href="#error-rates"></a>
</h2>
<p>In the following chapters, we’ll see different methods that all aim
to control for false positives in different settings.</p>
<p>The main distinction between these methods lies in the <em>error
rate</em> that they control for. We’ll get to know three error
rates:</p>
<ul>
<li>The comparison-wise error rate, where we use the p-value as we
obtain it from each individual test (no correction).</li>
<li>The family-wise error rate, where we like to avoid <em>any</em>
false positives.</li>
<li>The false-discovery rate, where we like to control the
<em>fraction</em> of false positives within our hits.</li>
</ul>
<p>We’ll see screening scenarios and multiple comparisons, and in both
settings, different methods exist to control for the different error
rates.</p>
<p>The plethora of methods can often be overwhelming. Therefore it’s
good to keep in mind that in practice, most of the time the workflow can
be broken down into three simple steps:</p>
<ol style="list-style-type: decimal">
<li>Clearly define your <strong>research question</strong>, and describe
the testing scenario.<br>
</li>
<li>Based on your research question, choose an appropriate <strong>error
rate</strong> that you need to control for.</li>
<li>Choose a <strong>method</strong> that controls for the selected
error rate that applies for the testing scenario at hand.</li>
</ol>
<p>Choosing the error rate is often a philosophical question. Before we
dive into the details of the individual methods, let’s get an overview
on the three error rates we’re considering. Which one to choose depends
on how the hypothesis tests in your scenario are connected.</p>
<figure><img src="fig/error-rates.png" alt="Which error rate should you control for?" class="figure mx-auto d-block"><div class="figcaption">Which error rate should you control for?</div>
</figure><ul>
<li>If each of these tests answers a separate research question, you can
apply the comparison-wise error rate, which means you don’t have to
correct anything.</li>
<li>Sometimes, one incorrect rejection changes the overall conclusion in
your research setting. For example, if you decide on the safety of a
chemical, which is a mixture of ingredients. You can ask for each
ingredient whether it’s safe (null hypothesis: it’s safe). If all null
hypohteses are true (all ingredients are safe), you’d like to conclude
that the entire chemical is safe. However, if you falsely reject one
individual null hypothesis, this will change the overall conclusion (you
falsely call the chemical unsafe). Therefore, you’d like to control the
probability of making any false rejections, which is called the
family-wise error rate.</li>
<li>In many scenarios, a few false positives won’t change the overall
conclusion. For example, in biological screens, we aim for a hit list of
candidate genes/proteins/substances that play a role in a process of
interest. We can then further verify candidates, or identify pathways
that are over-represented in the hits. For this, we can live with a few
false positives, but we’d like to make sure that <em>most of</em> what
we call a hit, actually <em>is</em> a hit. For such scenarios, we
control for the <em>false discovery rate</em>: the percentage of false
positives among the hits.</li>
</ul>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</div></section><section id="aio-Family-wise-error-rate"><p>Content from <a href="Family-wise-error-rate.html">Family-wise error rate</a></p>
<hr>
<p>Last updated on 2024-09-17 |

        <a href="https://example.com/FIXME-template-source/edit/main/episodes/Family-wise-error-rate.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is the family-wise error rate (FWER), and why is it important
in multiple testing scenarios?</li>
<li>How does the Bonferroni procedure adjust p-values to control the
FWER, and what are its limitations?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand the concept of the family-wise error rate (FWER) and its
significance in multiple testing, including the implications of making
multiple comparisons without controlling for FWER.</li>
<li>Understand the Bonferroni procedure for adjusting p-values to
maintain the FWER at a specified level, and recognize when alternative
methods may be more appropriate or effective in controlling for multiple
comparisons.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="a-multi-hypothesis-testing-framework">A Multi-Hypothesis Testing Framework<a class="anchor" aria-label="anchor" href="#a-multi-hypothesis-testing-framework"></a>
</h2>
<hr class="half-width">
<p>In multiple testing scenarios, we often have an overarching
hypothesis that encompasses several individual hypotheses, each
examining specific aspects or relationships within the data. This
approach allows us to explore various facets of the research question
comprehensively.</p>
<p>Going back to our study investigating the effects of air pollution on
the prevalence of a disease, the overarching hypothesis could be
formulated as follows:</p>
<p><strong>Exposure to air pollution is associated with increased
prevalence of the disease.</strong></p>
<p>Under this overarching null hypothesis, several individual hypotheses
can be formulated to examine different aspects of the relationship
between air pollution exposure and disease prevalence. These individual
hypotheses may focus on various pollutants, different health outcomes,
or specific populations.</p>
<p>An example using three individual null hypotheses:</p>
<p><strong><span class="math inline">\(H_{0,1}\)</span></strong>:
Exposure to particulate matter is not associated with increased disease
prevalence.</p>
<p><strong><span class="math inline">\(H_{0,2}\)</span></strong>:
Exposure to nitrogen dioxide is not associated with increased disease
prevalence.</p>
<p><strong><span class="math inline">\(H_{0,3}\)</span></strong>:
Long-term exposure to ozone (O3) is not associated with an increased
prevalence of the disease.</p>
<p>The three null hypotheses can be combined to the following overall
null hypothesis:</p>
<p><strong><span class="math inline">\(H_{0}\)</span></strong>: None of
the air pollutants is associated with an increased prevalence of the
disease.</p>
<div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<p>As soon as one of the individual null hypotheses is rejected, we also
reject the overall null hypothesis. Rejecting the overall null
hypothesis means that at least one of the individual null hypotheses is
false. Therefore, we will want to make sure that we have not a single
false positive among our individual hypothesis tests.</p>
</div>
</div>
</div>
<figure><img src="fig/overarching-hypothesis.png" alt="Relationship between Overall Hypothesis and Individual Hypotheses (Effects of Air Pollution on Disease Prevalence)" class="figure mx-auto d-block"><div class="figcaption">Relationship between Overall Hypothesis and
Individual Hypotheses (Effects of Air Pollution on Disease
Prevalence)</div>
</figure><p>Now, let us assume that after data collection, for hypothesis 1, we
find that 15 out of 100 individuals exposed to high levels of
particulate matter develop the disease, for hypothesis 2, 20 out of 100
individuals exposed to high levels of nitrogen dioxide develop the
disease and for hypothesis 3, 5 out of 100 individuals exposed to high
levels of ozone develop the disease.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span> <span class="op">=</span> <span class="fl">100</span> <span class="co"># number of test persons</span></span>
<span><span class="va">p</span> <span class="op">=</span> <span class="fl">0.04</span> <span class="co"># Known prevalence of the disease in the general population</span></span>
<span></span>
<span><span class="va">individuals_suffered</span> <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="fl">15</span>, <span class="fl">9</span>, <span class="fl">5</span><span class="op">)</span> <span class="co"># number of individuals who suffered from the disease for each hypothesis</span></span></code></pre>
</div>
<p>We can visualise this as follows:</p>
<!--

```r
complement <- n - individuals_suffered
data <- rbind(individuals_suffered, complement)
colnames(data) <- c("particles","nitrogene", "O3")
rownames(data) <- c("disease","healthy")

mosaicplot(t(data),main="")
```

<img src="fig/Family-wise-error-rate-rendered-unnamed-chunk-3-1.png" style="display: block; margin: auto;" />
-->
<figure><img src="fig/Family-wise-error-rate-rendered-unnamed-chunk-4-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>We can conduct binomial tests for each hypothesis and calculate the
p-values in R.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Calculate the p-values for each hypothesis using the binomial probability mass function</span></span>
<span><span class="va">p_values</span> <span class="op">=</span> <span class="fu">sapply</span><span class="op">(</span><span class="va">individuals_suffered</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">p_value</span> <span class="op">=</span> <span class="fu">binom.test</span><span class="op">(</span><span class="va">x</span>,n<span class="op">=</span><span class="va">n</span>,p<span class="op">=</span><span class="va">p</span><span class="op">)</span><span class="op">$</span><span class="va">p.value</span></span>
<span>  <span class="kw">return</span><span class="op">(</span><span class="va">p_value</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu">length</span><span class="op">(</span><span class="va">p_values</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu">cat</span><span class="op">(</span><span class="fu">sprintf</span><span class="op">(</span><span class="st">"Hypothesis %d: p = %.4f\n"</span>, <span class="va">i</span>, <span class="va">p_values</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="co"># Print the p-values for each hypothesis</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Hypothesis 1: p = 0.0000
Hypothesis 2: p = 0.0190
Hypothesis 3: p = 0.6033</code></pre>
</div>
</section><section><h2 class="section-heading" id="the-probability-of-having-at-least-one-false-positive-among-m-tests-fwer">The probability of having at least one false positive among <span class="math inline">\(m\)</span> tests (FWER)<a class="anchor" aria-label="anchor" href="#the-probability-of-having-at-least-one-false-positive-among-m-tests-fwer"></a>
</h2>
<hr class="half-width">
<p>Let us assume each test has a probability <span class="math inline">\(\alpha\)</span> of producing a false positive, and
that we have <span class="math inline">\(m\)</span> independent
tests.</p>
<p>The probability that a single test does not produce a false positive
is calculated as: <span class="math display">\[1-\alpha\]</span></p>
<p>Since the tests are independent, the probability that none of the
<span class="math inline">\(m\)</span> tests produces a false positive
is calculated as: <span class="math display">\[(1-\alpha)^m\]</span></p>
<p>Therefore, the probability of at least one false positive is the
complement of the probability that none of the tests produce a false
positive, and is calculated as: <span class="math display">\[P(\text{at
least one false positive})=1−(1−\alpha)^m\]</span></p>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<ol style="list-style-type: decimal">
<li>In the above example, we have three different tests. Will the
probability of making one false positive still be 0.05?</li>
<li>What can we do to decrease the probability of any false
positive?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li>We can use R to calculate the probability of at least one false
positive for our example with three tests:</li>
</ol>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m</span> <span class="op">&lt;-</span> <span class="fl">3</span></span>
<span><span class="va">alpha</span> <span class="op">&lt;-</span> <span class="fl">0.05</span></span>
<span></span>
<span><span class="fl">1</span> <span class="op">-</span> <span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">alpha</span><span class="op">)</span><span class="op">^</span><span class="va">m</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 0.142625</code></pre>
</div>
<ol start="2" style="list-style-type: decimal">
<li>By decreasing the alpha.</li>
</ol>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="the-bonferroni-correction">The Bonferroni correction<a class="anchor" aria-label="anchor" href="#the-bonferroni-correction"></a>
</h2>
<hr class="half-width">
<div class="section level3">
<h3 id="adjusting-the-significance-level">Adjusting the significance level<a class="anchor" aria-label="anchor" href="#adjusting-the-significance-level"></a>
</h3>
<p>Now, to control the probability of having any false positive within
the set of tests (also known as family-wise error rate or FWER), we can
use methods such as the Bonferroni correction.</p>
<p>The Bonferroni procedure adjusts the significance level for each
individual test by dividing the desired overall significance level by
the number of tests conducted (m).</p>
<p><span class="math display">\[\alpha_{\text{Bonf}}= \alpha/m
\]</span></p>
<p>Where:</p>
<p>α is the desired overall significance level.<br>
m is the number of hypothesis tests conducted.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">FWER</span> <span class="op">&lt;-</span> <span class="fl">0.05</span><span class="co"># Define the desired Family-wise error rate</span></span>
<span></span>
<span><span class="va">m</span> <span class="op">&lt;-</span> <span class="fu">length</span><span class="op">(</span><span class="va">p_values</span><span class="op">)</span><span class="co"># Calculate the number of hypothesis tests conducted (m)</span></span>
<span></span>
<span><span class="va">alpha_bonf</span> <span class="op">&lt;-</span> <span class="va">FWER</span> <span class="op">/</span> <span class="va">m</span> <span class="co"># Calculate Bonferroni adjusted significance level</span></span>
<span></span>
<span><span class="va">alpha_bonf</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 0.01666667</code></pre>
</div>
<p>Since in our example above we have three tests, the Bonferroni
corrected significance level is <span class="math inline">\(\alpha_{\text{bonf}} = 0.05/3 \approx
0.0167\)</span>.</p>
<p>In this lesson, we’ll not go into the proof for why this method
works. But we can convince ourselves that it does, by calculating the
FWER with the adjusted significance level, using the formula derived
above.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fl">1</span> <span class="op">-</span> <span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="fl">0.0167</span><span class="op">)</span><span class="op">^</span><span class="fl">3</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 0.04926799</code></pre>
</div>
<p>With the adjusted significance level of <span class="math inline">\(\alpha \approx 0.0167\)</span> for three tests, we
are back to the desired <span class="math inline">\(FWER\)</span> of
5%.</p>
<div class="section level4">
<h4 id="in-our-example">In our example<a class="anchor" aria-label="anchor" href="#in-our-example"></a>
</h4>
<p>For each individual test, we compare the calculated p-value to the
adjusted significance level. If the p-value is less than or equal to
0.0167, we reject the null hypothesis for that test.</p>
<p>Based on the Bonferroni correction, we reject the null hypothesis for
Hypotheses 1, indicating significant associations between particulate
matter with disease prevalence. However, for Hypothesis 2 (nitrogen
dioxide exposure) and 3 (ozone exposure), we fail to reject the null
hypothesis, suggesting no significant association with disease
prevalence at the adjusted significance level. This adjustment for
multiple testing helps control the overall probability of making at
least one false-positive error across all tests conducted.</p>
<p>In this example, the evidence supports associations between certain
air pollutants and disease prevalence, but not for others. For our
overall null hypothesis - “None of the pollutants is associated with the
disease” - this still means that we have to reject it.</p>
<p>After the correction with the Bonferroni procedure, we can now say
that found particulate matter to be associated with disease, on a 5%
family-wise error rate level. This means that under the assumption that
all three null hypothesis are false, we expect the chance of having at
least one false positive to be below 5%.</p>
<p>This finding suggests a nuanced interpretation wherein the
relationship between air pollution exposure and disease prevalence may
vary depending on the specific pollutant considered. Therefore, further
investigation and analysis may be necessary to fully elucidate the
relationship between air pollution exposure and disease prevalence and
to refine the overarching null hypothesis accordingly.</p>
</div>
</div>
<div class="section level3">
<h3 id="adjusting-the-p-value">Adjusting the p-value<a class="anchor" aria-label="anchor" href="#adjusting-the-p-value"></a>
</h3>
<p>Instead of changing the significance level, another (equivalent)
calculation is adjusting the p-values obtained from individual
hypothesis tests, followed by comparing the adjusted p-values to the
desired FWER. With this procedure, we can reject those individual null
hypotheses that have a p-value below the desired FWER (<span class="math inline">\(p &lt; \text{FWER}\)</span>).</p>
<p><span class="math display">\[p_{\text{Bonf}}= p \times m\]</span></p>
<div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<ol style="list-style-type: decimal">
<li><p>Look up <code>p.adjust</code>. This function adjusts P-values for
Multiple Comparisons.</p></li>
<li><p>Using our previous example, can you adjust the p-values obtained
from individual hypothesis tests?</p></li>
<li><p>Which of the individual hypotheses can will be rejected at a
FWER=0.05?</p></li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li>You can look up the function as follows:</li>
</ol>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">help</span><span class="op">(</span><span class="st">"p.adjust"</span><span class="op">)</span></span></code></pre>
</div>
<p>2.In R, one can adjust the p-values as follows:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">p.adjust</span><span class="op">(</span><span class="va">p_values</span>, method <span class="op">=</span> <span class="st">"bonferroni"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 3.244612e-05 5.697576e-02 1.000000e+00</code></pre>
</div>
<ol start="3" style="list-style-type: decimal">
<li>We can check which of the individual hypotheses will be rejected at
a FWER=0.05:</li>
</ol>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">p.adjust</span><span class="op">(</span><span class="va">p_values</span>, method <span class="op">=</span> <span class="st">"bonferroni"</span><span class="op">)</span> <span class="op">&lt;</span> <span class="fl">0.05</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1]  TRUE FALSE FALSE</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>The conclusion is the same as when changing the significance level
for each test.</p>
</div>
</section><section><h2 class="section-heading" id="family-wise-error-rate-and-power">Family-wise error rate and power<a class="anchor" aria-label="anchor" href="#family-wise-error-rate-and-power"></a>
</h2>
<hr class="half-width">
<div class="section level3">
<h3 id="what-is-statistical-power">What is statistical power?<a class="anchor" aria-label="anchor" href="#what-is-statistical-power"></a>
</h3>
<p>Statistical power is the probability of detecting an effect with a
statistical test, given that the effect really exists. Expressing this
with the terms that we learned in this tutorial, power is defined as</p>
<p><span class="math display">\[ P(\text{rejection} | \text{null
hypothesis is false}).\]</span></p>
<div id="challenge3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Suppose we test a set of hypotheses. How does applying the Bonferroni
correction impact the power of these tests?</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<p>The Bonferroni correction decreases the power of the test, especially
when we have many tests.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="the-number-of-tests-matters">The number of tests matters<a class="anchor" aria-label="anchor" href="#the-number-of-tests-matters"></a>
</h3>
<p>Remember that the adjusted significance level depends on the number
of tests that we conduct:</p>
<p><span class="math display">\[\alpha_{\text{Bonf}}= \alpha/m
\]</span></p>
<p>Let’s look at some adjusted significance levels for different numbers
of tests, given that we like to control the family-wise error rate at
<span class="math inline">\(\alpha_\text{FWER} = 0.05\)</span>:</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n_test</span> <span class="op">&lt;-</span> <span class="fu">seq</span><span class="op">(</span><span class="fl">1</span>,<span class="fl">20000</span><span class="op">)</span></span>
<span><span class="va">alpha_adjusted</span> <span class="op">&lt;-</span> <span class="fl">0.05</span><span class="op">/</span><span class="va">n_test</span></span>
<span></span>
<span><span class="fu">data.frame</span><span class="op">(</span><span class="va">n_test</span>,<span class="va">alpha_adjusted</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">n_test</span>, y<span class="op">=</span><span class="fu">log10</span><span class="op">(</span><span class="va">alpha_adjusted</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span><span class="op">)</span></span></code></pre>
</div>
<p><img src="fig/Family-wise-error-rate-rendered-unnamed-chunk-11-1.png" style="display: block; margin: auto;" class="figure">
We see that the adjusted <span class="math inline">\(\alpha\)</span> is
dropping quickly. For <span class="math inline">\(20000\)</span> tests,
which is a reasonable number in genomic screens, alpha will be:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">last</span><span class="op">(</span><span class="va">alpha_adjusted</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 2.5e-06</code></pre>
</div>
<p>We’ll only reject the null for tests that produce a p-value <span class="math inline">\(&lt;0.0000025\)</span>, which means the chances of
finding a hit are reduced drastically compared to before adjustment.</p>

<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</section></section><section id="aio-False-discovery-rate"><p>Content from <a href="False-discovery-rate.html">False discovery rate</a></p>
<hr>
<p>Last updated on 2024-09-17 |

        <a href="https://example.com/FIXME-template-source/edit/main/episodes/False-discovery-rate.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How does correcting for the family-wise error rate (FWER) affect the
number of significant hits in large-scale data, such as RNA-Seq analysis
of 20,000 human genes?</li>
<li>What is the interpretation of a p-value histogram?</li>
<li>How can the Benjamini-Hochberg method be applied to control the
false discovery rate (FDR) in RNA-Seq data, and what are the benefits of
using this method over FWER correction?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Introduce the concept of p-value histograms and explain their
interpretation.</li>
<li>Explain the Benjamini-Hochberg method for controlling the false
discovery rate (FDR).</li>
<li>Provide practical examples and R code to apply the
Benjamini-Hochberg method to RNA-Seq data.</li>
<li>Discuss the advantages of controlling the FDR over FWER in the
context of large-scale genomic data.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h1>
<p>In high-throughput experiments like RNA-Seq, we often conduct
thousands of statistical tests simultaneously. This large number of
tests increases the risk of false positives. While controlling the
family-wise error rate (FWER), the probability of making at least one
type I error, is one approach to address false positives, it can be too
conservative, leading to few or no significant results. An alternative
approach is to control the <strong>False Discovery Rate (FDR)</strong>,
the expected proportion of false positives among the rejected
hypotheses, which offers a balance between identifying true positives
and limiting false positives. In this tutorial, we will learn how each
method affects the outcome.</p>
<div class="section level2">
<h2 id="example-the-airway-dataset-in-r">Example: The Airway dataset in R<a class="anchor" aria-label="anchor" href="#example-the-airway-dataset-in-r"></a>
</h2>
<p>The Airway dataset contains gene expression data from a study
investigating the effects of dexamethasone (a corticosteroid medication)
on airway smooth muscle cells. The dataset is part of the
<code>airway</code> package in Bioconductor, a project that provides
tools for the analysis and comprehension of high-throughput genomic
data.</p>
<p>In differential expression analysis, thousands of statistical tests
are conducted: For each gene, one can test whether its expression is
different in cells with dexamethasone treatment, compared to cells
without treatment. If the expression differs between the two conditions,
we call the gene differentially expressed (DE). Like in the previous
example, we have a set up null hypotheses:</p>
<p><span class="math inline">\(H_{0,1}\)</span>: Gene 1 is not DE.</p>
<p><span class="math inline">\(H_{0,2}\)</span>: Gene 2 is not DE.</p>
<p>…</p>
<p><span class="math inline">\(H_{0,20000}\)</span>: Gene 20000 is not
DE.</p>
<p>For each gene, we run a test (similar to a t-test) comparing the two
treatment groups, which returns a p-value that summarizes the evidence
we have for the respective null hypothesis.</p>
<p>Unlike in the air pollution example, our question is not whether
<em>any</em> of the genes is DE, but rather <em>which ones</em>: We’d
like to come up with a hit list of genes that can be further
investigated.</p>
<p>In the following, we’ll work with a <code>data.frame</code> called
<code>gene_pvalues</code>, containing p-values for all genes in the
<code>airway</code> data set.</p>
<div id="coding-along" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="coding-along" class="callout-inner">
<h3 class="callout-title">Coding along</h3>
<div class="callout-content">
<p>If you’d like to follow the code that we run here, you can either</p>
<ul>
<li>run the differential expression analysis in <code>DESeq2</code> to
create the data or</li>
<li>download the p-values data.</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Create data using DESeq2 </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>This is how the p-values are created:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="va">DESeq2</span><span class="op">)</span></span>
<span><span class="kw">library</span><span class="op">(</span><span class="va">airway</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#Load the Airway Dataset</span></span>
<span><span class="fu">data</span><span class="op">(</span><span class="st">"airway"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#Create a DESeq2 dataset from the airway data</span></span>
<span><span class="va">dds</span> <span class="op">&lt;-</span> <span class="fu">DESeqDataSetFromMatrix</span><span class="op">(</span>countData <span class="op">=</span> <span class="fu">assay</span><span class="op">(</span><span class="va">airway</span><span class="op">)</span>,</span>
<span>                              colData <span class="op">=</span> <span class="fu">colData</span><span class="op">(</span><span class="va">airway</span><span class="op">)</span>,</span>
<span>                              design <span class="op">=</span> <span class="op">~</span> <span class="va">dex</span><span class="op">)</span><span class="co">#the design formula ~ dex specifies that we are interested in the effect of the treatment (dex), which indicates dexamethasone treatment.</span></span>
<span></span>
<span><span class="co"># Pre-filter the dataset to remove rows with low counts  _Pre-filter low counts</span></span>
<span><span class="va">keep</span> <span class="op">&lt;-</span> <span class="fu">rowSums</span><span class="op">(</span><span class="fu">counts</span><span class="op">(</span><span class="va">dds</span><span class="op">)</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">10</span></span>
<span><span class="va">dds</span> <span class="op">&lt;-</span> <span class="va">dds</span><span class="op">[</span><span class="va">keep</span>,<span class="op">]</span></span>
<span></span>
<span><span class="co">#Perform the DESeq2 analysis to find DEGs between treated and untreated samples</span></span>
<span><span class="va">dds</span> <span class="op">&lt;-</span> <span class="fu">DESeq</span><span class="op">(</span><span class="va">dds</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#Obtain the results of the differential expression analysis.</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu">results</span><span class="op">(</span><span class="va">dds</span><span class="op">)</span><span class="co">##By default, this will compare treated (dex) vs untreated samples</span></span>
<span></span>
<span><span class="co"># Extract the p-values into a separate variable</span></span>
<span><span class="va">pvalues</span> <span class="op">&lt;-</span> <span class="va">res</span><span class="op">$</span><span class="va">pvalue</span></span>
<span></span>
<span></span>
<span><span class="co"># Combine the gene names and their corresponding p-values</span></span>
<span><span class="va">gene_pvalues</span> <span class="op">&lt;-</span> <span class="fu">data.frame</span><span class="op">(</span>gene<span class="op">=</span><span class="fu">rownames</span><span class="op">(</span><span class="va">res</span><span class="op">)</span>, pvalue<span class="op">=</span><span class="va">res</span><span class="op">$</span><span class="va">pvalue</span><span class="op">)</span></span>
<span><span class="co"># Save to a CSV file</span></span>
<span><span class="fu">write.csv</span><span class="op">(</span><span class="va">gene_pvalues</span>, file<span class="op">=</span><span class="st">"data/DEG_pvalues.csv"</span>, row.names<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Load data </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>If you like to work with them without running the code, you can load
pre-computed p-values as follows:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">gene_pvalues</span> <span class="op">&lt;-</span> <span class="fu">read.csv</span><span class="op">(</span><span class="fu">url</span><span class="op">(</span><span class="st">"https://raw.githubusercontent.com/sarahkaspar/Multiple-Testing-tutorial-project/main/episodes/data/DEG_pvalues.csv"</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
</div>
</div>
</div>
</div>
<p>The table below shows the first six rows of the generated p-values
for each gene, the data which we are going use to see how using FWER and
FDR to controlling for false positives differ.</p>
<table class="table">
<caption>p-Values for each analysed gene</caption>
<thead><tr class="header">
<th align="left">gene</th>
<th align="right">pvalue</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">ENSG00000000003</td>
<td align="right">0.0286313</td>
</tr>
<tr class="even">
<td align="left">ENSG00000000419</td>
<td align="right">0.0422631</td>
</tr>
<tr class="odd">
<td align="left">ENSG00000000457</td>
<td align="right">0.7871234</td>
</tr>
<tr class="even">
<td align="left">ENSG00000000460</td>
<td align="right">0.6976522</td>
</tr>
<tr class="odd">
<td align="left">ENSG00000000971</td>
<td align="right">0.0886259</td>
</tr>
<tr class="even">
<td align="left">ENSG00000001036</td>
<td align="right">0.0428887</td>
</tr>
</tbody>
</table>
<p>20000 p-values are too many to list them all, but we can look at
their distribution by visualizing a p-value histogram. A p-value
histogram is a graphical representation that displays the distribution
of p-values obtained from multiple hypothesis tests. It can help us in
assessing the presence of true effects versus null effects and in
understanding the overall behavior of the tests. It can also help us to
better control for false positives. To understand how this works, we’ll
have to look into the theory. In the next section, we’ll learn</p>
<ul>
<li>that a p-value histogram is composed of two fractions: the null and
the alternative</li>
<li>what behavior we expect from the null fraction</li>
</ul>
<p>Understanding this will provide us with a tool for controlling the
False discovery rate.</p>
</div>
</div>
<div class="section level1">
<h1 id="the-theory-of-p-value-histograms">The theory of p-value Histograms<a class="anchor" aria-label="anchor" href="#the-theory-of-p-value-histograms"></a>
</h1>
<p>To create a p-value histogram, we plot the p-values on the x-axis,
typically ranging from 0 to 1. The y-axis represents the frequency (or
count) of p-values falling within specific bins (intervals) of the
x-axis. Let’s do this for the <code>airway</code> p-values.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">gene_pvalues</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">pvalue</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>binwidth<span class="op">=</span><span class="fl">0.01</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/False-discovery-rate-rendered-unnamed-chunk-3-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><div class="section level2">
<h2 id="a-p-value-histogram-is-composed-of-two-fractions">A p-value histogram is composed of two fractions<a class="anchor" aria-label="anchor" href="#a-p-value-histogram-is-composed-of-two-fractions"></a>
</h2>
<p>We can think of the p-value histogram as being composed of two
fractions: the alternative and the null fraction.</p>
<figure><img src="fig/composition-of-pvalues.png" alt="Figure credit Cecile LeSueur" class="figure mx-auto d-block"><div class="figcaption">Figure credit Cecile LeSueur</div>
</figure><p>Together, the alternative and null fraction give us a p-value
histogram as the one observed for the <code>airway</code> data.</p>
</div>
<div class="section level2">
<h2 id="why-are-these-our-expectations-of-the-null-and-alterative-fraction">Why are these our expectations of the null and alterative
fraction?<a class="anchor" aria-label="anchor" href="#why-are-these-our-expectations-of-the-null-and-alterative-fraction"></a>
</h2>
<p>Let’s start with the easier case: If we use a statistical test that
is good at detecting differentially expressed genes, then it will
produce low p-values for the DE genes, resulting in a peak close to 0.
Depending on the power of the test, the peak towards 0 is sharp (high
power, all the DE genes have low p-values) or flat and less pronounced
(low power, many DE genes have p-values &gt;&gt;0).</p>
<p>But why do we expect a uniform distribution of p-values that come
from genes where the null hypothesis is true? This comes from the
definition of p-values. We expect 5% of the p-values to be <span class="math inline">\(&lt;0.05\)</span>, 10% of the p-vales to be <span class="math inline">\(&lt;0.1\)</span>, etc. Unfortunately, this
definition is not extremely intuitive. Therefore, we use a simulation of
tests where the null hypothesis is true, and use it to recap the
definition of p-values.</p>
<p>We learned that in the t-test, the test statistic <span class="math inline">\(t\)</span> follows a <em>t-distribution</em> under
the null hypothesis. So, when the null hypothesis is true, we expect the
value of <span class="math inline">\(t\)</span> to be randomly drawn
from a t-distribution. For demonstration purposes, we can simulate 2000
draws from a t-distribution (here, I choose the degrees of freedom to be
<span class="math inline">\(\nu = 5\)</span>, which is an arbitrary
choice) and visualize their distribution.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">set.seed</span><span class="op">(</span><span class="fl">55</span><span class="op">)</span></span>
<span><span class="va">ts</span> <span class="op">&lt;-</span> <span class="fu">rt</span><span class="op">(</span><span class="fl">2000</span>, df<span class="op">=</span><span class="fl">5</span><span class="op">)</span></span>
<span><span class="fu">data.frame</span><span class="op">(</span>t<span class="op">=</span><span class="va">ts</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">t</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>binwidth<span class="op">=</span><span class="fl">0.2</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>title<span class="op">=</span><span class="st">"Null distribution of t"</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/False-discovery-rate-rendered-unnamed-chunk-4-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>This is our null distribution!</p>
<p>Since we’ll decide significance based on the absolute value of t,
<span class="math inline">\(|t|\)</span>, I’ll calculate it here:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">abs_t</span> <span class="op">&lt;-</span> <span class="fu">abs</span><span class="op">(</span><span class="va">ts</span><span class="op">)</span><span class="co"># take their absolute values</span></span>
<span><span class="fu">data.frame</span><span class="op">(</span>abs_t<span class="op">=</span><span class="va">abs_t</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">abs_t</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>binwidth<span class="op">=</span><span class="fl">0.1</span>, boundary<span class="op">=</span><span class="fl">1</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>title<span class="op">=</span><span class="st">"Null distribution of |t|"</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/False-discovery-rate-rendered-unnamed-chunk-5-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>This is our null distribution of absolute t-values. In hypothesis
testing, we ask: “What is the probability in the null distribution to
observe a value at least as extreme as the observed <span class="math inline">\(|t|\)</span>?” And to answer this question we look
at the <em>cumulative distribution</em> of <span class="math inline">\(|t|\)</span>. While in practice, we’ll look up the
theoretical cumulative distribution, we’ll here look at the cumulative
distribution of our simulation, hence an <em>empirical</em> cumulative
distribution.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">data.frame</span><span class="op">(</span>abs_t <span class="op">=</span> <span class="va">abs_t</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">abs_t</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">stat_ecdf</span><span class="op">(</span><span class="op">)</span></span></code></pre>
</div>
<p><img src="fig/False-discovery-rate-rendered-unnamed-chunk-6-1.png" style="display: block; margin: auto;" class="figure">
This cumulative distribution function answers the question “for a given
value of <span class="math inline">\(|t|\)</span>, how many other
elements of the simulation are <em>smaller than</em> this value?”. Which
is exactly the opposite of what we’re asking when calculating a p-value.
In fact, the p-value is defined as <span class="math inline">\(1-\text{CDF}(|t|)\)</span>, which looks like
this:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">data.frame</span><span class="op">(</span>abs_t <span class="op">=</span> <span class="va">abs_t</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">abs_t</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>y <span class="op">=</span> <span class="fl">1</span> <span class="op">-</span> <span class="va">..y..</span><span class="op">)</span>, stat<span class="op">=</span><span class="st">'ecdf'</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>y<span class="op">=</span><span class="st">"1-ECDF"</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_vline</span><span class="op">(</span>xintercept <span class="op">=</span><span class="fl">3.1</span>, color<span class="op">=</span><span class="st">"darkblue"</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_hline</span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0.03</span>, color<span class="op">=</span><span class="st">"darkblue"</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/False-discovery-rate-rendered-unnamed-chunk-7-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Code adapted from <a href="https://stackoverflow.com/questions/37221303/how-to-plot-reverse-complementary-ecdf-using-ggplot" class="external-link">here</a>.
So when we calculate a p-value, we (visually speaking) look up the
observed value of <span class="math inline">\(|t|\)</span>
(<code>abs_t</code>) on the x-axis, and match it to the corresponding
1-ECDF on the y-axis, which is the resulting <span class="math inline">\(p\)</span>.</p>
<p>For example, if we observe <span class="math inline">\(t=3.1\)</span>
(darkblue vertical line), then this corresponds to <span class="math inline">\(1-ECDF = 0.03\)</span> (darkblue horizontal line),
meaning that under the null distribution, <span class="math inline">\(=
3%\)</span> of the values are smaller than the observed <span class="math inline">\(t\)</span>. Our p-value for this test is <span class="math inline">\(p= 0.03\)</span>.</p>
<p>What does this tell us about the distribution of p-values? We could
go and slice the 1-ECDF into chunks of 5% of the data points:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">data.frame</span><span class="op">(</span>abs_t <span class="op">=</span> <span class="va">abs_t</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">abs_t</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>y <span class="op">=</span> <span class="fl">1</span> <span class="op">-</span> <span class="va">..y..</span><span class="op">)</span>, stat<span class="op">=</span><span class="st">'ecdf'</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>y<span class="op">=</span><span class="st">"1-ECDF"</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_hline</span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fu">seq</span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span>,by<span class="op">=</span><span class="fl">0.05</span><span class="op">)</span>,</span>
<span>             col<span class="op">=</span><span class="st">"gray"</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/False-discovery-rate-rendered-unnamed-chunk-8-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>The observations in each slice will make up one bin in the p-value
histogram which we’re going to create next.</p>
<p>All of these chunks should contain about 5% of the p-values -
<strong>by the definition of the p-value via the cumulative
distribution</strong>.</p>
<p>For instance, the upper 5% of the <span class="math inline">\(|t|\)</span>-values are be between <span class="math inline">\(2.6\)</span> and <span class="math inline">\(6.8\)</span>, so we give them p-values between
0.00 and 0.05. The next 5% of the <span class="math inline">\(|t|\)</span>-values will be between <span class="math inline">\(2.0\)</span> and <span class="math inline">\(2.6\)</span>, so we give them p-values between
0.05 and 0.10. And so on… resulting in the following p-value histogram
again sliced by 5%-bins for demonstration:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">data.frame</span><span class="op">(</span>abs_t <span class="op">=</span> <span class="va">abs_t</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>p <span class="op">=</span> <span class="fu">pt</span><span class="op">(</span><span class="va">abs_t</span>, df<span class="op">=</span><span class="fl">5</span>,lower.tail<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span><span class="op">*</span><span class="fl">2</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">p</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>binwidth<span class="op">=</span><span class="fl">0.05</span>, boundary<span class="op">=</span><span class="fl">0</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/False-discovery-rate-rendered-unnamed-chunk-10-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>We could also define the p-value via the empirical null
distribution:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">abs_t_vec</span> <span class="op">&lt;-</span> <span class="va">abs_t</span></span>
<span><span class="fu">data.frame</span><span class="op">(</span>abs_t <span class="op">=</span> <span class="va">abs_t</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">rowwise</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>p <span class="op">=</span> <span class="fu">mean</span><span class="op">(</span><span class="va">abs_t</span><span class="op">&gt;</span><span class="va">abs_t_vec</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">p</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>binwidth<span class="op">=</span><span class="fl">0.05</span>, boundary<span class="op">=</span><span class="fl">0</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/False-discovery-rate-rendered-unnamed-chunk-11-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Again, each 5% bin contains 5% (=100) of the p-values, because the
p-values are defined by the percentage of values of <span class="math inline">\(|t|\)</span> in the simulated null distribution
which are smaller than the observed one.</p>
<div class="section level3">
<h3 id="recap">Recap<a class="anchor" aria-label="anchor" href="#recap"></a>
</h3>
<ul>
<li>p-values are defined via the cumulative distribution of the test
statistic under the null hypothesis</li>
<li>They answer the question: what percentage of values is expected more
extreme than the observed one?</li>
<li>If we bin our data into equally-sized quantiles, we expect to see
roughly the same number of p-values in each quantile.</li>
<li>Therefore: In a set of tests where the null hypothesis is true, and
where the test statistic behaves like expected in theory, we will see a
uniform distribution of p-values</li>
<li>This works for any test statistic, <em>as long as we know its
distribution</em>.</li>
</ul>
</div>
</div>
</div>
<div class="section level1">
<h1 id="the-false-discovery-rate">The False Discovery Rate<a class="anchor" aria-label="anchor" href="#the-false-discovery-rate"></a>
</h1>
<p>Let’s come back to our p-value histogram of differentially expressed
genes. We learned that it is composed of null and alternative fraction,
and we learned that in theory, the null fraction is uniformly
distributed.</p>
<p>We can use this knowledge to estimate the fraction of false positives
among the positives at an arbitrary p-value cut-off. This is illustrated
below.</p>
<figure><img src="fig/p_value%20histogram%20decomposition.png" alt="A p-value histogram decomposition (adapted from MSMB)" class="figure mx-auto d-block"><div class="figcaption">A p-value histogram decomposition (adapted from
<a href="https://www.huber.embl.de/msmb/" class="external-link">MSMB</a>)</div>
</figure><p>Say, we determine some cut-off p-value, indicated by the red line.
Then we can visually estimate what percentage of tests right at this
threshold are false positives, namely by dividing the length of the
light red segment by the overall length of the line. That’s the local
<strong>fdr</strong>, and it applies to tests rejected just at this
threshold. Now we will probably reject all the tests below the
threshold, and to get the fraction of false positives within those, we
divide the dark gray area by the the total area to the left of the red
line. This is the capital letter <strong>FDR</strong> and it’s an
average property of all tests rejected below the threshold.</p>
<p>The false discovry rate is defined as <span class="math inline">\(FDR
= \frac{FP}{TP+FP}.\)</span></p>
<div class="section level2">
<h2 id="in-our-example">In our example<a class="anchor" aria-label="anchor" href="#in-our-example"></a>
</h2>
<p>Let’s come back to our example and assume a significance level <span class="math inline">\(\alpha=0.05\)</span>. If we use this p-value
cut-off, we can make a rough visual estimate of the <span class="math inline">\(FDR\)</span>.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">gene_pvalues</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">pvalue</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>binwidth <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_hline</span><span class="op">(</span>yintercept<span class="op">=</span><span class="fl">160</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_vline</span><span class="op">(</span>xintercept<span class="op">=</span><span class="fl">0.05</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/False-discovery-rate-rendered-unnamed-chunk-12-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Left to the vertical line, we have the area of the FP fraction (below
the horizontal line), and the TP fraction (above the horizontal line).
We can guess that around <span class="math inline">\(2/3\)</span> of the
hits are TP.</p>
</div>
<div class="section level2">
<h2 id="adjusted-p-values">Adjusted p-values<a class="anchor" aria-label="anchor" href="#adjusted-p-values"></a>
</h2>
<p>At any given p-value cut-off, we can estimate the FDR. So we could
take each p-values in our screen, use it as a cut-off and return the
estimated FDR. This is the adjusted p-value. It tells us the FDR that we
would have for our hits if we rejected the null for all genes with a
p-value equal to or lower than that for this gene. We can then decide to
call all tests a hit, for which the adjusted p-value is smaller than the
desired <span class="math inline">\(\alpha_\text{FWER}\)</span>.</p>
<div class="section level3">
<h3 id="trade-off-fp-and-fn">Trade-off FP and FN<a class="anchor" aria-label="anchor" href="#trade-off-fp-and-fn"></a>
</h3>
<p>If we set the threshold very low, we will have a low FDR (low
fraction of false positives), but will miss many of the differentially
expressed genes. When we move the red line to the right, that is
increase the threshold, we’ll capture more and more of the
differentially expressed genes, but at the cost of more and more false
positives, and if we go too far, we will almost exclusively admit more
False Positives.</p>
</div>
</div>
<div class="section level2">
<h2 id="controlling-fdr-using-the-benjamini-hochberg-method">Controlling FDR Using the Benjamini-Hochberg Method<a class="anchor" aria-label="anchor" href="#controlling-fdr-using-the-benjamini-hochberg-method"></a>
</h2>
<p>The Benjamini-Hochberg (BH) method is a statistical procedure used to
control the FDR in multiple hypothesis testing, where the chance of
obtaining false positives increases. The BH method helps control the
proportion of false positives (false discoveries) among the rejected
hypotheses, thus providing a more balanced approach than traditional
methods like the <strong>Bonferroni correction</strong>, which can be
overly conservative. The BH procedure adjusts the p-values from multiple
tests to control the FDR. These adjusted p-values can be compared to a
significance threshold to determine which results are significant.</p>
<div class="section level3">
<h3 id="steps-of-the-benjamini-hochberg-procedure">Steps of the Benjamini-Hochberg Procedure<a class="anchor" aria-label="anchor" href="#steps-of-the-benjamini-hochberg-procedure"></a>
</h3>
<p>First the observed p-values are arranged in ascending order. Next,
ranks are assigned to the p-values, with the smallest p-value getting
rank 1, the next smallest rank 2, and so on. Then, for each p-value, the
BH critical value is calculated as;</p>
<p><span class="math display">\[\text{BH critical value} = \frac{i}{m}
\times Q\]</span></p>
<p>where, i is the rank, m is the total number of tests, and Q is the
desired FDR level (e.g., 0.05).</p>
<p>After this calculation, the largest p-value that is less than or
equal to its BH critical value is identified. This p-value and all
smaller p-values are considered significant. Optionally, one can adjust
the p-values to reflect the BH correction using software functions.</p>
<p>Now, let us continue with the generated <code>p_values</code> to
apply the Benjamini-Hochberg correction in R, and also plot the adjusted
p values for comparison.</p>
<p>What the Benjamini-Hochberg algorithm does, is that it estimates the
null component, and finds the threshold below which we should reject for
a desired FDR. Equivalently, and that’s the way it’s implemented in R,
we can say it produces adjusted p-values. If we reject everything below
a certain adjusted p-value (say 5%), this will lead to an FDR of 5%,
meaning that 5% of the hits are estimated to be false-positives.</p>
</div>
</div>
</div>
<div class="section level1">
<h1 id="wrap-up">Wrap up<a class="anchor" aria-label="anchor" href="#wrap-up"></a>
</h1>
<p>We discussed that controlling the FDR trades some false positives for
the chances to discover more hits. Let’s compare the three error rates
that we encountered so far</p>
<ul>
<li>comparison-wise error rate</li>
<li>family-wise error rate</li>
<li>false discovery rate</li>
</ul>
<p>in terms of the number of hits they return.</p>
<p>We can check how many genes were significantly differentially
expressed between the treated and untreated cells at α=0.05 before we
apply any type 1 error correction method:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">gene_pvalues</span> <span class="op">&lt;-</span> <span class="fu">na.omit</span><span class="op">(</span><span class="va">gene_pvalues</span><span class="op">)</span>  <span class="co"># Removes rows with NA values</span></span>
<span><span class="fu">sum</span><span class="op">(</span><span class="va">gene_pvalues</span><span class="op">$</span><span class="va">pvalue</span> <span class="op">&lt;</span> <span class="fl">0.05</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 4646</code></pre>
</div>
<p>Now, if we proceed to apply the Bonferroni method to correct the
family-wise error rate (FWER), we can then determine the number of genes
that show significant differential expression after the correction.</p>
<div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<ul>
<li>Use <code>p.adjust</code> to calculate adjusted p-values using
Benjamini Hochberg. How many hits do you get if you control the FDR at
10%?</li>
<li>Do the same with Bonferroni</li>
<li>What fraction of false positives do you estimate if we use a
comparison-wise error rate with <span class="math inline">\(\alpha=0.5\)</span>?</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">alpha</span><span class="op">=</span><span class="fl">0.1</span></span>
<span><span class="co"># Apply Benjamini-Hochberg correction</span></span>
<span><span class="va">p_adjusted_BH</span> <span class="op">&lt;-</span> <span class="fu">p.adjust</span><span class="op">(</span><span class="va">gene_pvalues</span><span class="op">$</span><span class="va">pvalue</span>, method <span class="op">=</span> <span class="st">"BH"</span><span class="op">)</span></span>
<span><span class="va">significant_bh</span> <span class="op">&lt;-</span> <span class="va">p_adjusted_BH</span> <span class="op">&lt;</span> <span class="va">alpha</span></span>
<span><span class="va">Benjamini_Hochberg_genes</span><span class="op">&lt;-</span><span class="fu">sum</span><span class="op">(</span><span class="va">significant_bh</span><span class="op">)</span> <span class="co"># Number of significant hits after Benjamini-Hochberg correction</span></span>
<span><span class="va">Benjamini_Hochberg_genes</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 3234</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">alpha</span><span class="op">=</span><span class="fl">0.1</span></span>
<span><span class="co"># Apply Bonferroni correction</span></span>
<span><span class="va">p_adjusted</span> <span class="op">&lt;-</span> <span class="fu">p.adjust</span><span class="op">(</span><span class="va">gene_pvalues</span><span class="op">$</span><span class="va">pvalue</span>, method <span class="op">=</span> <span class="st">"bonferroni"</span><span class="op">)</span></span>
<span><span class="va">significant_bonferroni</span><span class="op">&lt;-</span> <span class="va">p_adjusted</span> <span class="op">&lt;</span> <span class="va">alpha</span></span>
<span><span class="va">bonferroni_genes</span><span class="op">&lt;-</span><span class="fu">sum</span><span class="op">(</span><span class="va">significant_bonferroni</span><span class="op">)</span> <span class="co"># Number of significant hits after bonferroni correction</span></span>
<span><span class="va">bonferroni_genes</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 797</code></pre>
</div>
<p>Estimated fraction of FP at <span class="math inline">\(\alpha=0.5\)</span>:</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">data.frame</span><span class="op">(</span>p<span class="op">=</span><span class="va">gene_pvalues</span><span class="op">$</span><span class="va">pvalue</span> ,padj <span class="op">=</span><span class="va">p_adjusted_BH</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">filter</span><span class="op">(</span><span class="va">p</span><span class="op">&lt;</span><span class="fl">0.05</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">pull</span><span class="op">(</span><span class="va">padj</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">max</span><span class="op">(</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 0.2362799</code></pre>
</div>
<p>This is in line with our visual estimate.</p>
</div>
</div>
</div>
</div>
<div class="section level2">
<h2 id="what-control-method-should-we-choose">What control method should we choose?<a class="anchor" aria-label="anchor" href="#what-control-method-should-we-choose"></a>
</h2>
<p>It is essential to understand the difference between controlling the
False Discovery Rate (FDR) and the Family-Wise Error Rate (FWER) as each
applies to different research scenarios.</p>
<p>Controlling the FDR is suitable in situations where it is often
acceptable to have some false positives, such as in large-scale
biological screenings (e.g., genomics, proteomics). The goal is to
identify as many true positives as possible, knowing that some false
positives can be filtered out in subsequent validation steps.</p>
<p>On the other hand, controlling FWER is suitable in high-precision
scenarios requiring strict control of false positives. In scenarios
where false positives can have significant consequences (e.g., clinical
trials, diagnostic tests), it is crucial to minimize even a single false
positive. Here, FWER control is more appropriate because it ensures a
lower risk of making any type I error.</p>
<p>Therefore, it is important to understand that in exploratory
research, where the aim is to generate hypotheses or discover potential
leads, FDR control is preferred due to its higher power. In confirmatory
research, where results need to be highly reliable, FWER control is more
suitable.</p>
<div class="section level3">
<h3 id="take-home-message">Take home message<a class="anchor" aria-label="anchor" href="#take-home-message"></a>
</h3>
<p>While FDR control is widely used in biological research due to its
balance between discovery and false positive control, FWER control is
crucial in settings where the cost of even a single false positive is
high. Each method serves its purpose depending on the specific goals and
acceptable risk levels of the research context. Understanding the
trade-offs between these approaches allows researchers to choose the
most appropriate method for their specific scenario.</p>
</div>
<div class="section level3">
<h3 id="more-on-p-value-histograms">More on p-value histograms<a class="anchor" aria-label="anchor" href="#more-on-p-value-histograms"></a>
</h3>
<p>If you encounter p-value histograms that don’t fit the expectation we
just discussed, have a look at this <a href="http://varianceexplained.org/statistics/interpreting-pvalue-histogram/" class="external-link">post
in varianceexplained.org</a></p>
</div>
</div>
<div class="section level2">
<h2 id="further-reading">Further reading<a class="anchor" aria-label="anchor" href="#further-reading"></a>
</h2>
<ul>
<li><a href="%22http://varianceexplained.org/statistics/interpreting-pvalue-histogram/%22" class="external-link">How
to interpret a p-value histogram</a></li>
<li><a href="%22https://www.huber.embl.de/msmb/06-chap.html%22" class="external-link">MSMB</a></li>
</ul>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</div></section><section id="aio-Pairwise-comparisons"><p>Content from <a href="Pairwise-comparisons.html">Pairwise comparisons</a></p>
<hr>
<p>Last updated on 2024-09-17 |

        <a href="https://example.com/FIXME-template-source/edit/main/episodes/Pairwise-comparisons.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are pairwise comparisons, and how do they relate to the broader
concept of multiple testing in statistical analysis?</li>
<li>How can we effectively conduct and interpret pairwise comparisons to
make valid statistical inferences while controlling for the family-wise
error rate?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand the Concept of Pairwise Comparisons</li>
<li>Learn how to conduct and interpret Pairwise Comparisons</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="pairwise-comparisons">Pairwise comparisons<a class="anchor" aria-label="anchor" href="#pairwise-comparisons"></a>
</h1>
<p>Pairwise comparisons are a fundamental concept in statistical
analysis, allowing us to compare the means or proportions of multiple
groups or conditions. In this episode, we will explore the concept of
pairwise comparisons, their importance in statistical testing, and how
they relate to the broader context of multiple testing.</p>
<div class="section level2">
<h2 id="an-example-air-pollution-in-four-different-exposure-groups">An example: Air pollution in four different exposure groups<a class="anchor" aria-label="anchor" href="#an-example-air-pollution-in-four-different-exposure-groups"></a>
</h2>
<p>In our example of air pollution, fine particulate matter and other
pollutants can induce systemic inflammation in the body, leading to an
increase in circulating inflammatory markers like C-reactive protein
(CRP). Suppose we decide to divide the population into four groups based
on different levels of exposure to air pollution. Lets assume these
groups are as follow:</p>
<ul>
<li>Low exposure: Individuals living in rural areas with minimal
industrial activity and low traffic density.</li>
<li>Moderate exposure: Individuals living in suburban areas with some
industrial activity and moderate traffic density.</li>
<li>High exposure: Individuals living in urban areas with significant
industrial activity and high traffic density.</li>
<li>Very high exposure: Individuals living near industrial zones, major
highways, or heavily polluted urban areas.</li>
</ul>
<p>The table below shows the first six rows of CRP values of individuals
in the four exposure groups.</p>
<table class="table">
<caption>CRP values of individuals in the four exposure groups</caption>
<thead><tr class="header">
<th align="right">CRP</th>
<th align="left">Exposure</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">1.379049</td>
<td align="left">Low</td>
</tr>
<tr class="even">
<td align="right">2.039645</td>
<td align="left">Low</td>
</tr>
<tr class="odd">
<td align="right">5.617417</td>
<td align="left">Low</td>
</tr>
<tr class="even">
<td align="right">2.641017</td>
<td align="left">Low</td>
</tr>
<tr class="odd">
<td align="right">2.758576</td>
<td align="left">Low</td>
</tr>
<tr class="even">
<td align="right">5.930130</td>
<td align="left">Low</td>
</tr>
</tbody>
</table>
<p>In this case the “Exposure” column is a categorical variable with
four groups of exposure levels (<code>Low</code>, <code>Moderate</code>,
<code>High</code>, and <code>Very_high</code>).</p>
<div id="coding-along" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="coding-along" class="callout-inner">
<h3 class="callout-title">Coding along</h3>
<div class="callout-content">
<p>If you like to reproduce the example we use here, you can download
the data as follows:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">exp_data</span> <span class="op">&lt;-</span> <span class="fu">read_csv</span><span class="op">(</span>file<span class="op">=</span><span class="fu">url</span><span class="op">(</span><span class="st">"https://raw.githubusercontent.com/sarahkaspar/Multiple-Testing-tutorial-project/main/episodes/data/exposure_data.csv"</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
</div>
</div>
</div>
<p>Here, we visualize the data as a boxplot, together with the
individual data points:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Visualize the data</span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="va">exp_data</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">Exposure</span>, y <span class="op">=</span> <span class="va">CRP</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_boxplot</span><span class="op">(</span>coef <span class="op">=</span> <span class="fl">5</span>, fill <span class="op">=</span> <span class="st">"lightblue"</span>, alpha <span class="op">=</span> <span class="fl">0.25</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span>position <span class="op">=</span> <span class="fu">position_jitter</span><span class="op">(</span>width <span class="op">=</span> <span class="fl">.1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Boxplot of Four Exposure Groups"</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/Pairwise-comparisons-rendered-unnamed-chunk-2-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><div id="pairwise-comparisons-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="pairwise-comparisons-1" class="callout-inner">
<h3 class="callout-title">Pairwise comparisons</h3>
<div class="callout-content">
<p>In the above example:</p>
<ul>
<li>How many pairwise comparisons are possible?</li>
<li>What is a suitable test for making these comparisons?</li>
<li>If we choose <span class="math inline">\(\alpha =0.05\)</span>, what
is the probability of seeing at least one significant difference, if in
fact all differences are 0?</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<ul>
<li>6 pairwise comparisons are possible.</li>
<li>t-test</li>
<li><span class="math inline">\(1-(0.95)^6 \approx 0.26\)</span></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level2">
<h2 id="defining-the-question">Defining the question<a class="anchor" aria-label="anchor" href="#defining-the-question"></a>
</h2>
<p>Generally, from the above plot, as the exposure level increases from
low to very high, we might expect to see a corresponding increase in the
median exposure level and higher median C-reactive protein (CRP)
values.</p>
<p>On this type of data, we can run different types of analyses,
depending on the question we ask. Let’s look at two different
scenarios:</p>
<ul>
<li><p><strong>Scenario 1:</strong> This is an exploratory analysis. We
are interested in finding out whether there is any difference between
the four exposure groups at all.</p></li>
<li><p><strong>Scenario 2:</strong> We have one or more particular
exposure groups of interest, which we like to compare to the low
exposure group. In this case, the low exposure group serves as a
reference.</p></li>
</ul>
<div class="section level3">
<h3 id="scenario-1-anova">Scenario 1: ANOVA<a class="anchor" aria-label="anchor" href="#scenario-1-anova"></a>
</h3>
<p>In this scenario, we’re basically comparing all against all.</p>
<p>Our <strong>null hypothesis</strong> is that there are not any
differences in CRP amounts in the different groups. From the challenge
above, we learned that 6 individual comparisons are not unlikely to
produce at least one false positive. In case that there are no
differences at all, this probability is estimated to be <span class="math inline">\(P=0.26\)</span>.</p>
<p>Recall that in this situation, we like to control for the family-wise
error rate. Our null hypothesis is that there’s nothing going on at all,
and once we reject the null for one of our individual comparisons, we
also reject this overall null hypothesis.</p>
<p>In this kind of situation, the ANOVA F-test is the method of choice.
It is a method that</p>
<ul>
<li>is used to compare means between different groups and</li>
<li>controls for the family-wise error rate.</li>
<li>It’s applicable when the data points are normally distributed around
the group means and the variances are similar among the groups. This
appears to be the case in our example.</li>
</ul>
<p>In general, a one-way ANOVA for <span class="math inline">\(t\)</span> groups tests the null hypothesis:</p>
<p><span class="math display">\[H_0: \mu_1 = \mu_2 = ... =
\mu_t,\]</span> where <span class="math inline">\(\mu_i\)</span> is the
mean of population <span class="math inline">\(i\)</span>.</p>
<p>The alternative hypothesis is:</p>
<p><span class="math display">\[H_A: \mu_i \neq \mu_j \text{ for some i
and j where i}\neq \text{j}\]</span></p>
<p>For this, we use an F-statistic:</p>
<p><span class="math display">\[F= \frac{\text{between group
variance}}{\text{within group variance}}\]</span> <span class="math inline">\(F\)</span> becomes large, if the groups differ
strongly in their group means, while the variance within the groups is
small. ANOVA is closely related to the t-test. In fact, if we perform an
ANOVA on a data set with two groups only, it is equivalent to performing
a t-test. For more details on ANOVA, see <a href="https://online.stat.psu.edu/stat500/lesson/10/10.2" class="external-link">the PennState
online materials</a>.</p>
<p>We can perform an ANOVA in R as follows:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">anova_result</span> <span class="op">&lt;-</span> <span class="fu">aov</span><span class="op">(</span><span class="va">CRP</span> <span class="op">~</span> <span class="va">Exposure</span>, data <span class="op">=</span> <span class="va">exp_data</span><span class="op">)</span></span>
<span><span class="fu">summary</span><span class="op">(</span><span class="va">anova_result</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>             Df Sum Sq Mean Sq F value  Pr(&gt;F)
Exposure      3   60.8  20.268   5.754 0.00086 ***
Residuals   196  690.4   3.523
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
<p>The ANOVA output above shows statistically significant differences
among the means of the four exposure groups. We therefore reject the
null hypothesis of equal means.</p>
</div>
<div class="section level3">
<h3 id="post-hoc-tests">Post-hoc tests<a class="anchor" aria-label="anchor" href="#post-hoc-tests"></a>
</h3>
<p>ANOVA alone does not tell us which specific groups differ
significantly from each other. To determine this, we need to conduct
further analysis. Post-hoc tests are follow up procedures, used to
compare pairs of groups in order to identify where the differences lie.
These tests provide more detailed information about which specific group
means are different from each other.</p>
<p>Parametric methods, such as Tukey’s HSD, are used when assumptions of
normality and homogeneity of variance are met. Non-parametric methods,
such as pairwise Wilcoxon tests, are suitable when data do not meet the
assumptions of parametric tests. In R, performing pairwise comparisons
is straightforward using built-in functions and packages. We perform a
Tukey post-hoc test using <code>TukeyHSD()</code> to determine which
specific exposure group means differ significantly from each other.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tukey_result</span> <span class="op">&lt;-</span> <span class="fu">TukeyHSD</span><span class="op">(</span><span class="va">anova_result</span><span class="op">)</span></span>
<span><span class="va">tukey_result</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = CRP ~ Exposure, data = exp_data)

$Exposure
                         diff        lwr       upr     p adj
Low-High           -0.4233920 -1.3960536 0.5492695 0.6727404
Moderate-High       0.3006174 -0.6720442 1.2732790 0.8539510
Very_high-High      1.0854145  0.1127530 2.0580761 0.0219956
Moderate-Low        0.7240094 -0.2486521 1.6966710 0.2193881
Very_high-Low       1.5088066  0.5361450 2.4814681 0.0004795
Very_high-Moderate  0.7847971 -0.1878644 1.7574587 0.1597266</code></pre>
</div>
<p>The results provide pairwise comparisons of group means including</p>
<ul>
<li>
<code>diff</code>: the estimated difference between the two
groups</li>
<li>the lower (<code>lrw</code>) and upper (<code>upr</code>) bounds of
a 95% family-wise confidence interval and</li>
<li>
<code>p adj</code>: the adjusted p-value which can be used to
control for the FWER.</li>
</ul>
<p>You often read that the Tukey procedure calculates <em>simultaneous
confidence intervals</em>, which implies that the confidence intervals
are dependent on how many comparisons we make. They grow (more
uncertainty), the more comparisons we make, and they can also be used to
determine whether two groups are significantly different. In the plot
below, each comparison has an estimate, and a confidence interval. We
call the difference significant with a 95% family-wise confidence level,
if the confidence interval <em>excludes 0</em>.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="va">multcompView</span><span class="op">)</span></span>
<span><span class="fu">par</span><span class="op">(</span>mar<span class="op">=</span><span class="fu">c</span><span class="op">(</span><span class="fl">5</span>,<span class="fl">6</span>,<span class="fl">4</span>,<span class="fl">1</span><span class="op">)</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">tukey_result</span>, </span>
<span>     las<span class="op">=</span><span class="fl">1</span>,</span>
<span>     cex.axis<span class="op">=</span><span class="fl">0.7</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/Pairwise-comparisons-rendered-unnamed-chunk-4-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure>
</div>
<div class="section level3">
<h3 id="interpreting-pairwise-comparison-results">Interpreting Pairwise Comparison Results<a class="anchor" aria-label="anchor" href="#interpreting-pairwise-comparison-results"></a>
</h3>
<p>Adjusted p-values reflect the probability of observing a given result
(or more extreme) under the assumption that all null hypotheses are
true, while accounting for the number of comparisons made. Here, we
interpret pairwise comparison results with adjusted p-values from the
Tukey test by assessing whether the adjusted p-value is less than the
chosen significance level (α) (0.05). If the adjusted p-value is below
the significance level, we conclude that the observed difference is
statistically significant after correcting for multiple comparisons.</p>
<p>In our example above, we observe significant differences in CRP
level</p>
<ul>
<li>between the exposure groups <code>very_high</code> and
<code>high</code>
</li>
<li>between the exposure groups <code>very_high</code> and
<code>low</code>
</li>
</ul>
<p>Consistent with that, the respective confidence intervals of these
two comparisons exclude zero.</p>
</div>
</div>
<div class="section level2">
<h2 id="scenario-2">Scenario 2<a class="anchor" aria-label="anchor" href="#scenario-2"></a>
</h2>
<p>In the second scenario, we aim to determine risk areas of increased
CRP concentration. Our control are low exposure regions, and we aim to
find out which of the other groups differ significantly from the control
in terms of CRP concentration, which classifies them as risk
regions.</p>
<p>If this is our question, in theory, we don’t need all six
comparisons. We just want to compare each group to the control. In
biology, this is a common scenario. We might have a small screen, where
we test several substances against a control, and need to determine
which of these has an effect. In this scenario, Tukey is overly
conservative, because it corrects for more tests than we actually like
to perform. We can, however apply the <em>Dunnett procedure</em>,
which</p>
<ul>
<li>controls for the FWER<br>
</li>
<li>in a scenario where we’d like to compare a number of groups to a
control.</li>
</ul>
<p>We can run the Dunnett test in R by using the function
<code>DunnettTest</code>. As input, we supply</p>
<ul>
<li>
<code>x</code>: a numeric vector of data values, or a list of
numeric data vectors</li>
<li>
<code>g</code>: a vector or factor object giving the group for the
corresponding elements of x</li>
<li>
<code>control</code>: the level of the control group against which
the others should be tested</li>
</ul>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="va">DescTools</span><span class="op">)</span></span>
<span><span class="va">dunnett</span> <span class="op">&lt;-</span> <span class="fu">DunnettTest</span><span class="op">(</span>x <span class="op">=</span><span class="va">exp_data</span><span class="op">$</span><span class="va">CRP</span>, </span>
<span>            g<span class="op">=</span> <span class="va">exp_data</span><span class="op">$</span><span class="va">Exposure</span>, </span>
<span>            control <span class="op">=</span> <span class="st">"Low"</span><span class="op">)</span></span>
<span><span class="va">dunnett</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
  Dunnett's test for comparing several treatments with a control :
    95% family-wise confidence level

$Low
                   diff     lwr.ci   upr.ci    pval
High-Low      0.4233920 -0.4652856 1.312070 0.53610
Moderate-Low  0.7240094 -0.1646682 1.612687 0.13711
Very_high-Low 1.5088066  0.6201290 2.397484 0.00023 ***

---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">par</span><span class="op">(</span>mar<span class="op">=</span><span class="fu">c</span><span class="op">(</span><span class="fl">5.1</span>, <span class="fl">7.1</span>, <span class="fl">4.1</span>, <span class="fl">1.1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">dunnett</span>, las<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/Pairwise-comparisons-rendered-unnamed-chunk-6-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><div id="discussion" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion" class="callout-inner">
<h3 class="callout-title">Discussion</h3>
<div class="callout-content">
<p>Why can’t we just run t-tests and then apply Bonferoni?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>This would not be wrong. It doesn’t produce confidence intervals
though. It’s sometimes a matter of convention.</p>
</div>
</div>
</div>
</div>
<div id="be-honest" class="callout testimonial">
<div class="callout-square">
<i class="callout-icon" data-feather="heart"></i>
</div>
<div id="be-honest" class="callout-inner">
<h3 class="callout-title">Be honest!</h3>
<div class="callout-content">
<p>In some cases, Dunnett is the way to go. But take a minute and think
about whether you’re not interested in the other comparisons as well.
For instance, when studying air pollution in different groups, you might
also be interested in whether there are differences between moderate and
high exposure. The fact that we don’t <em>see</em> large differences
between those two groups in the data is <em>not</em> a good reason for
leaving out this comparison!</p>
</div>
</div>
</div>
</div>
<div class="section level2">
<h2 id="pretty-figures">Pretty figures<a class="anchor" aria-label="anchor" href="#pretty-figures"></a>
</h2>
<p>In publications, you often find p-values plotted next to the
comparison in a figure. These plots can be produced in R.</p>
<div id="publication-ready-plots" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="publication-ready-plots" class="callout-inner">
<h3 class="callout-title">Publication-ready plots</h3>
<div class="callout-content">
<p>The R package <code>ggpubr</code> comes with the promise of producing
publication-ready plots. Let’s explore some of the functions and
practice our plotting skills. We use the <code>PlantGrowth</code> data
set.</p>
<ol style="list-style-type: decimal">
<li>Explore the data set. Which variables are there?</li>
<li>Use the following template to create a meaningful plot, comparing
the groups in the data set using ANOVA. You can look up the function
<code>stat_compare_means</code> in the R help section.</li>
</ol>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="va">ggpubr</span><span class="op">)</span> <span class="co"># load the required library</span></span>
<span></span>
<span><span class="va">PlantGrowth</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span>, y<span class="op">=</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_jitter</span><span class="op">(</span>width<span class="op">=</span><span class="fl">0.1</span><span class="op">)</span><span class="op">+</span> <span class="co"># feel free to replace by, or add, another type of plot</span></span>
<span>  <span class="fu">stat_compare_means</span><span class="op">(</span><span class="op">)</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li><p>Consult <code>?PlantGrowth</code>, or check
<code>names(PlantGrowth)</code>. The data set compares the biomass
(<code>weight</code>) in a control and two treatment
conditions.</p></li>
<li>
</li>
</ol>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="va">ggpubr</span><span class="op">)</span></span>
<span><span class="va">PlantGrowth</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">group</span>, y<span class="op">=</span><span class="va">weight</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_jitter</span><span class="op">(</span>width<span class="op">=</span><span class="fl">0.1</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">stat_compare_means</span><span class="op">(</span>method<span class="op">=</span><span class="st">"anova"</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/Pairwise-comparisons-rendered-unnamed-chunk-8-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>The <code>stat_compare_means</code> function infers the comparisons
to be made from the <code>x</code> and <code>y</code> that we supplied
in the <code>aes</code>.</p>
</div>
</div>
</div>
</div>
<p>We saw that the <code>stat_compare_means</code> function allows you
to conveniently add the p-value of an ANOVA in the plot. We can also add
the p-values for individual comparisons:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">PlantGrowth</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">group</span>, y<span class="op">=</span><span class="va">weight</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_jitter</span><span class="op">(</span>width<span class="op">=</span><span class="fl">0.1</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">stat_compare_means</span><span class="op">(</span>method<span class="op">=</span><span class="st">"anova"</span>, label.y <span class="op">=</span> <span class="fl">7</span><span class="op">)</span><span class="op">+</span> </span>
<span>  <span class="fu">stat_compare_means</span><span class="op">(</span>comparisons<span class="op">=</span><span class="fu">list</span><span class="op">(</span> <span class="fu">c</span><span class="op">(</span><span class="st">"ctrl"</span>, <span class="st">"trt1"</span><span class="op">)</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"ctrl"</span>, <span class="st">"trt2"</span><span class="op">)</span><span class="op">)</span>, method<span class="op">=</span><span class="st">"t.test"</span><span class="op">)</span>  </span></code></pre>
</div>
<figure><img src="fig/Pairwise-comparisons-rendered-unnamed-chunk-9-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Explanations on the code above:</p>
<ul>
<li>The setting <code>label.y = 7</code> shifts the result of the ANOVA
to a height of <code>weight=7</code>, so it won’t overlap with the
individual results.</li>
<li>If we self-define the comparisons, we can do this via the
<code>comparisons</code> argument by supplying a list of the comparisons
to be made.</li>
</ul>
<p>For more examples with the package, look up <a href="http://www.sthda.com/english/articles/24-ggpubr-publication-ready-plots/76-add-p-values-and-significance-levels-to-ggplots/" class="external-link">this
page</a>.</p>
</div>
<div class="section level2">
<h2 id="p-value-hacking-and-data-snooping">p-value hacking and data snooping<a class="anchor" aria-label="anchor" href="#p-value-hacking-and-data-snooping"></a>
</h2>
<p>We learned in this lesson that performing a series of tests increases
the chances of having at least one false positive. Therefore, it is
important to be transparent about which tests were conducted during a
study. If we run a large screen and reported only the tests that came
out as hits, we’re skipping the type-I error control, and for someone
else it is not comprehensible how much the results can be trusted. In
fact, we could perform a screen of 1000 biological samples, where there
are <em>no</em> true positives. Still, if we run each test at <span class="math inline">\(\alpha=0.05\)</span>, we would have around 50
hits, which are all false positives. The practice of analyzing data
extensively, testing multiple hypotheses, or making comparisons until
finding a statistically significant result or an interesting pattern, is
called <em>p-value hacking</em>.</p>
<p>But even if this is not your intention, you may still sometimes be
tempted to <em>snoop</em> in the data, which is a more subtle way of
increasing type I error.</p>
<p>Reconsider the air pollution example above:</p>
<figure><img src="fig/Pairwise-comparisons-rendered-unnamed-chunk-10-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>In this example, we might visually inspect our plot, and decide to
perform only the comparison between very high and low. Why? Because it
<em>looks</em> most promising. With only one test performed, there is
also no possibility to perform correction for multiple testing, which
may seem sound, while in reality, you performed all the pairwise
comparisons by eye, without correcting for them. Therefore, unless you
<em>pre-specified</em> one particular test <em>before</em> you inspected
the data, all tests should be performed and reported. Otherwise, this
approach will lead to inflated Type I error rates.</p>
</div>
<div class="section level2">
<h2 id="additional-resources">Additional resources<a class="anchor" aria-label="anchor" href="#additional-resources"></a>
</h2>
<p>For further reading and hands-on practice with pairwise comparisons
in R, refer to <a href="%22https://www.rdocumentation.org/%22" class="external-link">R
documentation on pairwise comparisons</a>, <a href="%22https://www.coursera.org/specializations/statistics%22" class="external-link">Applied
Statistics with R</a> and <a href="%22http://users.stat.umn.edu/~gary/book/fcdae.pdf%22" class="external-link">book by
Oehlert 2010</a> resources.</p>

<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</div></section><section id="aio-Summary"><p>Content from <a href="Summary.html">Summary</a></p>
<hr>
<p>Last updated on 2024-09-17 |

        <a href="https://example.com/FIXME-template-source/edit/main/episodes/Summary.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="section level1">
<h1 id="error-rates">Error rates<a class="anchor" aria-label="anchor" href="#error-rates"></a>
</h1>
<p>We have seen that when dealing with multiple testing, it is crucial
to understand the different types of error rates that can be controlled.
<strong>Comparison-Wise Error Rate (CWER)</strong>, which is the
probability of making a Type I error (false positive) in any single
hypothesis test, does not account for the multiplicity of tests and is
only applicable when each performed test answers a separate research
question. <strong>Family-Wise Error Rate (FWER)</strong>, the
probability of making at least one Type I error among all the hypothesis
tests, is a stringent error rate control method, suitable for situations
where even one false positive is highly problematic. This is the case
when we have one overarching null hypothesis that is rejected, as soon
as we reject one inidiviual null hypothesis. Methods like the Bonferroni
correction and Holm’s are often used to control FWER. <strong>False
Discovery Rate (FDR)</strong>, the expected proportion of Type I errors
among the rejected hypotheses, is less conservative than FWER and is
useful in large-scale testing scenarios, like genomic studies, where
some false positives are tolerable in exchange for higher power to
detect true positives. The Benjamini-Hochberg procedure is a common
method to control the FDR.</p>
<p>The choice of error rate to control depends on the research question
and the context of the study.</p>
</div>
<div class="section level1">
<h1 id="a-cook-book-to-navigate-multiple-testing-effectively">A Cook book to navigate multiple testing effectively<a class="anchor" aria-label="anchor" href="#a-cook-book-to-navigate-multiple-testing-effectively"></a>
</h1>
<p>To navigate multiple testing effectively, we should:</p>
<ol style="list-style-type: decimal">
<li><p>start by <strong>clearly defining our research question</strong>,
stating the specific hypothesis or set of hypotheses we aim to test, and
considering the implications of potential false positives and false
negatives.</p></li>
<li><p>Next, we <strong>decide on the appropriate error rate to
control</strong>, based on our research context and tolerance for Type I
errors. This means assessing how critical it is to avoid any false
positives (favoring FWER) versus allowing some false positives to
increase detection power (favoring FDR).</p></li>
<li><p>Finally, we <strong>select a suitable statistical method that
controls our chosen error rate</strong> and aligns with our data type
and experimental design, ensuring it is well-suited to the specifics of
our study.</p></li>
</ol>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div></section><section id="aio-References"><p>Content from <a href="References.html">References and Further Reading</a></p>
<hr>
<p>Last updated on 2024-09-17 |

        <a href="https://example.com/FIXME-template-source/edit/main/episodes/References.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="section level1">
<h1 id="books">Books<a class="anchor" aria-label="anchor" href="#books"></a>
</h1>
<ul>
<li>Oehlert, Gary W. A first course in design and analysis of
experiments. 2010. (<a href="https://conservancy.umn.edu/bitstream/handle/11299/168002/A%20First%20Course%20in%20Design%20and%20Analysis%20of%20Experiments_OehlertG_2010.pdf?sequence=1" class="external-link">pdf</a>)</li>
<li>Holmes, Susan Huber, and Wolfgang Huber. Modern statistics for
modern biology. Cambridge university press, 2018. (<a href="https://www.huber.embl.de/msmb/06-chap.html" class="external-link">link</a>)</li>
</ul>
</div>
<div class="section level1">
<h1 id="online-resources">Online Resources<a class="anchor" aria-label="anchor" href="#online-resources"></a>
</h1>
<ul>
<li>Post on p-value histograms on <a href="http://varianceexplained.org/statistics/interpreting-pvalue-histogram/" class="external-link">varianceexplained.org</a>
</li>
<li>PennState Eberly College of Science materials on ANOVA (<a href="https://online.stat.psu.edu/stat500/lesson/10" class="external-link">link</a>)</li>
</ul>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://example.com/FIXME-template-source/edit/main/README.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://example.com/FIXME-template-source/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://example.com/FIXME-template-source/" class="external-link">Source</a></p>
				<p><a href="https://example.com/FIXME-template-source/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:sarah.kaspar@embl.de">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.7" class="external-link">sandpaper (0.16.7)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.6" class="external-link">pegboard (0.7.6)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.4" class="external-link">varnish (1.0.4)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://FIXME-template-source.github.io/NA/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, statistics",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://FIXME-template-source.github.io/NA/aio.html",
  "identifier": "https://FIXME-template-source.github.io/NA/aio.html",
  "dateCreated": "2024-04-18",
  "dateModified": "2024-09-17",
  "datePublished": "2024-09-17"
}

  </script><script>
		feather.replace();
	</script>
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

